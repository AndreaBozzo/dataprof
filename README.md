# DataProfiler CLI - Un Tool Rust Innovativo per l'Analisi dei Dati

## ğŸ¯ Visione del Progetto

DataProfiler Ã¨ un CLI tool **in sviluppo** scritto in Rust che fornisce analisi istantanea e profonda dei dati, con un focus su **usabilitÃ **, 
**performance** e **insights azionabili**. A differenza di tool esistenti come qsv che sono suite complete, DataProfiler si concentra 
esclusivamente sul profiling con output ricchi e immediatamente utilizzabili.

## ğŸš€ Caratteristiche Innovative

### 1. **Smart Profiling con Pattern Recognition**
- **Pattern Detection Automatico**: Riconosce automaticamente pattern nei dati (email, telefoni, codici fiscali, IBAN, etc.)
- **Business Rules Inference**: Deduce regole di business dai dati (es. "il campo prezzo Ã¨ sempre > 0 e < 10000")
- **Anomaly Detection**: Identifica outlier e anomalie usando algoritmi statistici avanzati
- **Data Quality Score**: Assegna un punteggio di qualitÃ  per ogni campo e per il dataset completo

### 2. **Visualizzazione nel Terminale**
- **Mini-grafici ASCII**: Istogrammi, box plot e distribuzioni direttamente nel terminale
- **Heatmap di correlazioni**: Visualizza correlazioni tra campi numerici
- **Timeline visualization**: Per dati temporali, mostra pattern nel tempo
- **Color-coded output**: Usa colori per evidenziare problemi e insights

### 3. **Multi-formato con Performance Ottimizzata**
- Supporto nativo per CSV, TSV, JSON, JSONL, Parquet, Excel
- **Streaming mode**: Analizza file enormi senza caricarli in memoria
- **Sampling intelligente**: Per file molto grandi, usa sampling statisticamente significativo
- **Parallel processing**: Sfrutta tutti i core per analisi velocissime

### 4. **Export Ricchi e Actionable**
- **Report HTML interattivo**: Con grafici D3.js e tabelle filtrabili
- **JSON Schema generation**: Genera schema validazione con regole dedotte
- **SQL DDL generation**: Crea automaticamente CREATE TABLE statements
- **Data Dictionary Markdown**: Documentazione auto-generata
- **PowerBI/Tableau metadata**: Export diretto per tool BI

### 5. **Integrazione con il tuo Workflow**
- **Git-friendly output**: Report comparabili per tracking changes
- **CI/CD integration**: Exit codes significativi e output parsabile
- **Watch mode**: Monitora file per changes e ri-profila automaticamente
- **Pipeline mode**: Si integra facilmente in data pipeline esistenti

## ğŸ“Š FunzionalitÃ  Core

### Analisi Statistica Avanzata
```bash
# Profiling base con statistiche descrittive
dataprof analyze data.csv

# Profiling dettagliato con pattern recognition
dataprof analyze data.csv --deep

# Solo summary veloce
dataprof summary data.csv

# Confronto tra due dataset
dataprof compare before.csv after.csv
```

### Output Esempio
```
ğŸ“Š DataProfiler Report - sales_data.csv
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ˆ Dataset Overview:
â”œâ”€ Records: 1,234,567
â”œâ”€ Fields: 15
â”œâ”€ Size: 125.4 MB
â”œâ”€ Encoding: UTF-8
â””â”€ Quality Score: 92/100 âœ…

ğŸ” Field Analysis:

[1] customer_id (String)
â”œâ”€ Unique: 234,567 (19.0%)
â”œâ”€ Pattern: "CUS-[0-9]{6}" (99.8% match)
â”œâ”€ Missing: 12 (0.001%)
â””â”€ ğŸ“Š Distribution:
    CUS-100000-200000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45%
    CUS-200001-300000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     35%
    CUS-300001-400000: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        20%

[2] order_date (Date)
â”œâ”€ Format: YYYY-MM-DD (100%)
â”œâ”€ Range: 2023-01-01 to 2024-12-31
â”œâ”€ Missing: 0 (0%)
â”œâ”€ Anomalies: 3 future dates detected âš ï¸
â””â”€ ğŸ“ˆ Trend:
    2023 Q1: â–ˆâ–ˆâ–ˆâ–ˆ     25%
    2023 Q2: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   35%
    2023 Q3: â–ˆâ–ˆâ–ˆâ–ˆ     22%
    2023 Q4: â–ˆâ–ˆâ–ˆ      18%

[3] amount (Decimal)
â”œâ”€ Min: 0.01 | Max: 9,999.99 | Avg: 156.78
â”œâ”€ Std Dev: 245.67
â”œâ”€ Outliers: 234 values > 3Ïƒ (0.02%)
â”œâ”€ Currency Pattern: $X,XXX.XX detected
â””â”€ ğŸ“Š Distribution:
    $0-50:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 45%
    $51-100:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       25%
    $101-500:   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        20%
    $500+:      â–ˆâ–ˆ           10%

ğŸ”— Correlations Detected:
â”œâ”€ order_date â†” seasonal_campaign (0.78)
â”œâ”€ customer_tier â†” avg_order_value (0.65)
â””â”€ shipping_method â†” delivery_time (-0.82)

âš ï¸  Data Quality Issues:
â”œâ”€ [MEDIUM] 3 future dates in order_date
â”œâ”€ [LOW] 12 missing customer_ids
â””â”€ [INFO] Consider indexing on customer_id (high cardinality)

ğŸ’¡ Insights:
â”œâ”€ Seasonal pattern detected: 35% spike in Q2
â”œâ”€ Customer segmentation opportunity: 3 distinct value clusters
â””â”€ Potential data entry issue: amounts ending in .99 (45%)
```

## ğŸ› ï¸ Architettura Tecnica

### Stack Tecnologico
- **Parser Multi-formato**: 
  - `arrow-rs` per Parquet/Arrow
  - `polars` per operazioni DataFrame velocissime
  - `calamine` per Excel
  - `simd-json` per JSON parsing ultra-veloce

- **Analisi Statistica**:
  - `statrs` per statistiche avanzate
  - `ndarray` per operazioni matriciali
  - Custom algorithms per pattern detection

- **Visualizzazione**:
  - `ratatui` per TUI interattiva
  - `plotters` per grafici
  - `comfy-table` per tabelle formattate

- **Performance**:
  - `rayon` per parallelizzazione
  - `dashmap` per concurrent hashmaps
  - `memmap2` per file grandi

### Struttura del Progetto
```
dataprof/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs              # CLI entry point
â”‚   â”œâ”€â”€ analyzer/            # Core analysis engine
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ statistical.rs   # Statistical analysis
â”‚   â”‚   â”œâ”€â”€ patterns.rs      # Pattern recognition
â”‚   â”‚   â”œâ”€â”€ quality.rs       # Data quality scoring
â”‚   â”‚   â””â”€â”€ correlations.rs  # Correlation analysis
â”‚   â”œâ”€â”€ parsers/            # File format parsers
â”‚   â”‚   â”œâ”€â”€ csv.rs
â”‚   â”‚   â”œâ”€â”€ parquet.rs
â”‚   â”‚   â”œâ”€â”€ json.rs
â”‚   â”‚   â””â”€â”€ excel.rs
â”‚   â”œâ”€â”€ visualizers/        # Output formatters
â”‚   â”‚   â”œâ”€â”€ terminal.rs     # Terminal output
â”‚   â”‚   â”œâ”€â”€ html.rs         # HTML reports
â”‚   â”‚   â””â”€â”€ charts.rs       # Chart generation
â”‚   â”œâ”€â”€ exporters/          # Export formats
â”‚   â”‚   â”œâ”€â”€ schema.rs       # JSON Schema
â”‚   â”‚   â”œâ”€â”€ sql.rs          # SQL DDL
â”‚   â”‚   â””â”€â”€ metadata.rs     # BI tool metadata
â”‚   â””â”€â”€ utils/              # Utilities
â”‚       â”œâ”€â”€ sampling.rs     # Smart sampling
â”‚       â”œâ”€â”€ streaming.rs    # Streaming processing
â”‚       â””â”€â”€ progress.rs     # Progress bars
â”œâ”€â”€ tests/
â”œâ”€â”€ benches/                # Performance benchmarks
â””â”€â”€ examples/               # Usage examples
```

## ğŸš¦ Roadmap di Sviluppo

### Fase 1: MVP (2-3 settimane)
1. **Core Engine**
   - [ ] Parser CSV con streaming
   - [ ] Analisi statistica base (min, max, mean, std)
   - [ ] Type inference
   - [ ] Output tabellare nel terminale

2. **Pattern Detection Base**
   - [ ] Email, telefoni, date
   - [ ] Numeric patterns
   - [ ] Missing data analysis

### Fase 2: Features Avanzate (4-6 settimane)
1. **Multi-formato**
   - [ ] Parquet support
   - [ ] JSON/JSONL support
   - [ ] Excel support

2. **Visualizzazioni**
   - [ ] ASCII charts
   - [ ] Correlation matrix
   - [ ] HTML report generation

3. **Quality & Patterns**
   - [ ] Anomaly detection
   - [ ] Business rules inference
   - [ ] Data quality scoring

### Fase 3: Professional Features (6-8 settimane)
1. **Performance**
   - [ ] Parallel processing
   - [ ] Smart sampling per file enormi
   - [ ] Memory-mapped processing

2. **Integrations**
   - [ ] JSON Schema export
   - [ ] SQL DDL generation
   - [ ] BI tools metadata
   - [ ] CI/CD mode

3. **Advanced**
   - [ ] Watch mode
   - [ ] Comparison mode
   - [ ] Custom rules engine

## ğŸ’» Esempi di Utilizzo

### Caso 1: Analisi Veloce per Data Scientist
```bash
# Analisi rapida di un CSV
dataprof analyze sales_2024.csv --output-format terminal

# Export schema per validazione
dataprof analyze sales_2024.csv --export-schema sales_schema.json

# Genera DDL per database
dataprof analyze sales_2024.csv --export-sql postgres > create_table.sql
```

### Caso 2: Pipeline di Data Quality
```bash
# In uno script di CI/CD
if dataprof check data.csv --rules quality_rules.yaml; then
    echo "Data quality check passed"
    # Procedi con ETL
else
    echo "Data quality issues found"
    exit 1
fi
```

### Caso 3: Monitoring Continuo
```bash
# Watch mode per monitorare changes
dataprof watch incoming_data/ --on-change "notify-slack.sh"
```

### Caso 4: Report per Business Users
```bash
# Genera report HTML interattivo
dataprof analyze sales.csv --deep --output sales_report.html

# Genera data dictionary
dataprof document sales.csv --format markdown > data_dictionary.md
```

## ğŸ”§ Configurazione

```yaml
# .dataprof.yaml
analysis:
  sampling:
    enabled: true
    max_rows: 1_000_000
    confidence: 0.95
  
  patterns:
    custom:
      - name: "Italian Fiscal Code"
        regex: "^[A-Z]{6}[0-9]{2}[A-Z][0-9]{2}[A-Z][0-9]{3}[A-Z]$"
      - name: "Order ID"
        regex: "^ORD-[0-9]{8}$"
  
  quality:
    thresholds:
      missing_data_warning: 0.05
      missing_data_error: 0.10
      cardinality_check: true

output:
  terminal:
    colors: true
    charts: true
    max_width: 120
  
  html:
    template: "corporate"
    include_raw_data: false
```

## ğŸ¯ Vantaggi Competitivi

1. **Focus sul Profiling**: Non cerca di fare tutto come qsv, ma fa una cosa molto bene
2. **Output Business-Ready**: Report che possono essere condivisi direttamente con stakeholder
3. **Developer-Friendly**: Integrazione facile in pipeline esistenti
4. **Performance**: Sfrutta Rust per analisi velocissime anche su dataset enormi
5. **Intelligenza Built-in**: Pattern recognition e anomaly detection automatici

## ğŸ Getting Started

### Quick Start per Development
```bash
# Clone e setup
git clone https://github.com/tuousername/dataprof
cd dataprof

# Build
cargo build --release

# Run tests
cargo test

# Try it out
./target/release/dataprof analyze examples/sales_data.csv
```

### Prime Features da Implementare
1. **CSV Parser con Streaming** (2-3 giorni)
2. **Statistical Analysis Base** (2-3 giorni)
3. **Terminal Output Formatter** (1-2 giorni)
4. **Pattern Detection per Email/Phone** (2-3 giorni)
5. **Basic HTML Report** (2-3 giorni)

## ğŸ“š Risorse e Ispirazioni

- **Performance**: Prendere spunto da qsv per le ottimizzazioni
- **Patterns**: Libreria di regex comuni per data italiana/europea
- **Visualizations**: Guardare `bottom` e `htop` per TUI best practices
- **Reports**: Ispirarsi a pandas-profiling per completezza

## ğŸ¤ Contributi alla Community

Una volta stabile, il progetto potrebbe:
- Diventare una libreria riusabile oltre che CLI
- Integrarsi con progetti esistenti come Polars
- Fornire bindings per Python/R
- Creare plugin per IDE/editor

Questo progetto ha il potenziale per diventare il go-to tool per data profiling in Rust, colmando il gap tra tool command-line basilari e suite complete ma complesse.