//! Script generation for ML preprocessing pipelines
//!
//! This module is used by the database command to generate preprocessing scripts.
//! Functions may appear unused when testing without the database feature enabled.

#![allow(dead_code)]

use anyhow::Result;
use std::collections::{HashMap, HashSet};
use std::fs;
use std::path::{Path, PathBuf};

use dataprof::analysis::{
    MlReadinessLevel, MlReadinessScore, MlRecommendation, RecommendationPriority,
};

/// Generates a complete preprocessing Python script from ML recommendations
pub fn generate_preprocessing_script(
    ml_score: &MlReadinessScore,
    output_path: &Path,
    data_file_path: &str,
) -> Result<()> {
    let script_content = create_preprocessing_script_content(ml_score, data_file_path)?;
    fs::write(output_path, script_content)?;
    Ok(())
}

/// Creates the content of a preprocessing Python script
fn create_preprocessing_script_content(
    ml_score: &MlReadinessScore,
    data_file_path: &str,
) -> Result<String> {
    let mut script = String::new();

    // Add header comment
    script.push_str(&format!(
        r#"#!/usr/bin/env python3
"""
Preprocessing Script for ML Pipeline
Generated by DataProf ML Readiness Assessment

Source data: {}
Overall ML Readiness: {:.1}% ({})
Generated {} recommendations with actionable code snippets
"""

"#,
        data_file_path,
        ml_score.overall_score,
        readiness_level_to_string(&ml_score.readiness_level),
        ml_score.recommendations.len()
    ));

    // Collect all imports
    let imports = collect_unique_imports(&ml_score.recommendations);
    for import in &imports {
        script.push_str(&format!("{}\n", import));
    }
    script.push('\n');

    // Add main function
    script.push_str(&format!(
        r#"def preprocess_data(input_file=r"{}"):
    """
    Preprocess data for ML based on DataProf recommendations.

    Args:
        input_file (str): Path to the input CSV file

    Returns:
        pandas.DataFrame: Preprocessed data ready for ML
    """
    print("üîÑ Loading data...")
    df = pd.read_csv(input_file)
    print(f"üìä Loaded data: {{df.shape[0]}} rows, {{df.shape[1]}} columns")

    original_shape = df.shape

"#,
        data_file_path
    ));

    // Add preprocessing steps from recommendations
    let mut step_number = 1;

    // Group recommendations by priority for better organization
    let critical_recs: Vec<_> = ml_score
        .recommendations
        .iter()
        .filter(|r| matches!(r.priority, RecommendationPriority::Critical))
        .collect();
    let high_recs: Vec<_> = ml_score
        .recommendations
        .iter()
        .filter(|r| matches!(r.priority, RecommendationPriority::High))
        .collect();
    let medium_recs: Vec<_> = ml_score
        .recommendations
        .iter()
        .filter(|r| matches!(r.priority, RecommendationPriority::Medium))
        .collect();

    if !critical_recs.is_empty() {
        script.push_str("    # ========== CRITICAL ISSUES (Must Fix) ==========\n");
        for rec in critical_recs {
            if let Some(code) = &rec.code_snippet {
                script.push_str(&format!(
                    "    # Step {}: {} ({})\n",
                    step_number,
                    rec.category,
                    priority_to_string(&rec.priority)
                ));
                script.push_str(&format!("    # {}\n", rec.description));
                script.push_str(&format!(
                    "    print(f\"‚ö†Ô∏è Step {}: {}\")\n",
                    step_number, rec.category
                ));

                // Indent the code properly and handle newlines
                let indented_code = indent_code(code, 4);
                script.push_str(&format!("    {}\n", indented_code));
                script.push_str("    \n");
                step_number += 1;
            }
        }
    }

    if !high_recs.is_empty() {
        script.push_str("    # ========== HIGH PRIORITY ==========\n");
        for rec in high_recs {
            if let Some(code) = &rec.code_snippet {
                script.push_str(&format!(
                    "    # Step {}: {} ({})\n",
                    step_number,
                    rec.category,
                    priority_to_string(&rec.priority)
                ));
                script.push_str(&format!("    # {}\n", rec.description));
                script.push_str(&format!(
                    "    print(f\"üîß Step {}: {}\")\n",
                    step_number, rec.category
                ));

                let indented_code = indent_code(code, 4);
                script.push_str(&format!("    {}\n", indented_code));
                script.push_str("    \n");
                step_number += 1;
            }
        }
    }

    if !medium_recs.is_empty() {
        script.push_str("    # ========== MEDIUM PRIORITY ==========\n");
        for rec in medium_recs {
            if let Some(code) = &rec.code_snippet {
                script.push_str(&format!(
                    "    # Step {}: {} ({})\n",
                    step_number,
                    rec.category,
                    priority_to_string(&rec.priority)
                ));
                script.push_str(&format!("    # {}\n", rec.description));
                script.push_str(&format!(
                    "    print(f\"üîÑ Step {}: {}\")\n",
                    step_number, rec.category
                ));

                let indented_code = indent_code(code, 4);
                script.push_str(&format!("    {}\n", indented_code));
                script.push_str("    \n");
                step_number += 1;
            }
        }
    }

    // Add return statement and summary
    script.push_str(&format!(
        r#"    final_shape = df.shape
    print(f"‚úÖ Preprocessing complete!")
    print(f"üìä Final data shape: {{final_shape[0]}} rows, {{final_shape[1]}} columns")
    print(f"üîÑ Shape change: {{original_shape}} ‚Üí {{final_shape}}")

    return df


def main():
    """Main function to run the preprocessing pipeline."""
    try:
        # Run preprocessing
        processed_df = preprocess_data()

        # Save processed data
        output_file = r"{}_preprocessed.csv"
        processed_df.to_csv(output_file, index=False)
        print(f"üíæ Saved preprocessed data to: {{output_file}}")

        # Display basic info about processed data
        print("\nüìã Processed Data Summary:")
        print(processed_df.info())

        if processed_df.shape[1] <= 20:  # Show describe for manageable number of columns
            print("\nüìä Statistical Summary:")
            print(processed_df.describe())

    except Exception as e:
        print(f"‚ùå Error during preprocessing: {{e}}")
        raise


if __name__ == "__main__":
    main()
"#,
        data_file_path.replace(".csv", "")
    ));

    Ok(script)
}

/// Collect unique imports from all recommendations
fn collect_unique_imports(recommendations: &[MlRecommendation]) -> Vec<String> {
    let mut imports_set = HashSet::new();

    for rec in recommendations {
        for import in &rec.imports {
            imports_set.insert(import.clone());
        }
    }

    let mut imports: Vec<String> = imports_set.into_iter().collect();
    imports.sort();
    imports
}

/// Convert RecommendationPriority enum to string
fn priority_to_string(priority: &RecommendationPriority) -> String {
    match priority {
        RecommendationPriority::Critical => "critical".to_string(),
        RecommendationPriority::High => "high".to_string(),
        RecommendationPriority::Medium => "medium".to_string(),
        RecommendationPriority::Low => "low".to_string(),
    }
}

/// Convert MlReadinessLevel enum to string
fn readiness_level_to_string(level: &MlReadinessLevel) -> String {
    match level {
        MlReadinessLevel::Ready => "Ready".to_string(),
        MlReadinessLevel::Good => "Good".to_string(),
        MlReadinessLevel::NeedsWork => "Needs Work".to_string(),
        MlReadinessLevel::NotReady => "Not Ready".to_string(),
    }
}

/// Indent code properly for Python functions
fn indent_code(code: &str, spaces: usize) -> String {
    let indent = " ".repeat(spaces);
    code.replace("\\n", "\n") // Convert escaped newlines to actual newlines
        .lines()
        .map(|line| {
            if line.trim().is_empty() {
                String::new()
            } else {
                format!("{}{}", indent, line)
            }
        })
        .collect::<Vec<_>>()
        .join("\n")
}

/// Generates a batch preprocessing Python script for multiple files
pub fn generate_batch_preprocessing_script(
    ml_scores: &HashMap<PathBuf, MlReadinessScore>,
    output_path: &Path,
    base_path: &str,
) -> Result<()> {
    let script_content = create_batch_preprocessing_script_content(ml_scores, base_path)?;
    fs::write(output_path, script_content)?;
    Ok(())
}

/// Creates content for a batch preprocessing Python script
fn create_batch_preprocessing_script_content(
    ml_scores: &HashMap<PathBuf, MlReadinessScore>,
    base_path: &str,
) -> Result<String> {
    let mut script = String::new();

    // Calculate overall statistics
    let total_files = ml_scores.len();
    let avg_score = if total_files > 0 {
        ml_scores
            .values()
            .map(|s| s.overall_score as f32)
            .sum::<f32>()
            / total_files as f32
    } else {
        0.0
    };

    // Aggregate all recommendations by type and priority
    let aggregated_recommendations = aggregate_recommendations(ml_scores);

    // Add header comment
    script.push_str(&format!(
        r#"#!/usr/bin/env python3
"""
Batch Preprocessing Script for ML Pipeline
Generated by DataProf Batch Analysis

Source directory: {}
Total files analyzed: {}
Average ML Readiness: {:.1}%
Aggregated {} unique recommendation types across all files

This script processes multiple files in batch and applies consistent
preprocessing steps based on the most common recommendations found.
"""

"#,
        base_path,
        total_files,
        avg_score,
        aggregated_recommendations.len()
    ));

    // Collect all unique imports
    let imports = collect_batch_imports(ml_scores);
    for import in &imports {
        script.push_str(&format!("{}\n", import));
    }
    script.push_str("import os\nimport glob\nfrom pathlib import Path\n\n");

    // Add batch processing configuration
    script.push_str(&format!(
        r#"# Batch processing configuration
BATCH_CONFIG = {{
    'base_path': r"{}",
    'file_patterns': ['*.csv', '*.json'],
    'output_suffix': '_preprocessed',
    'parallel_processing': True,
    'max_workers': 4,
}}

"#,
        base_path
    ));

    // Add common preprocessing functions based on aggregated recommendations
    script.push_str(&generate_common_preprocessing_functions(
        &aggregated_recommendations,
    ));

    // Add main batch processing function
    script.push_str(&format!(
        r#"def preprocess_batch(config=None):
    """
    Process multiple files in batch with consistent preprocessing.

    Args:
        config (dict, optional): Batch processing configuration

    Returns:
        dict: Results summary with processed file paths and statistics
    """
    if config is None:
        config = BATCH_CONFIG

    base_path = config['base_path']
    print(f"üîÑ Starting batch preprocessing from: {{base_path}}")

    # Find all matching files
    all_files = []
    for pattern in config['file_patterns']:
        pattern_path = os.path.join(base_path, '**', pattern)
        files = glob.glob(pattern_path, recursive=True)
        all_files.extend(files)

    print(f"üìä Found {{len(all_files)}} files to process")

    if not all_files:
        print("‚ùå No files found matching the specified patterns")
        return {{'processed_files': [], 'total_processed': 0, 'errors': []}}

    results = {{
        'processed_files': [],
        'total_processed': 0,
        'errors': [],
        'summary': {{}}
    }}

    # Process files
    if config.get('parallel_processing', False) and len(all_files) > 1:
        results = process_files_parallel(all_files, config)
    else:
        results = process_files_sequential(all_files, config)

    # Generate summary report
    generate_batch_summary(results)

    return results

def process_files_sequential(files, config):
    """Process files one by one (safer for large files)."""
    results = {{'processed_files': [], 'total_processed': 0, 'errors': []}}

    for i, file_path in enumerate(files, 1):
        print(f"\\nüîÑ Processing file {{i}}/{{len(files)}}: {{Path(file_path).name}}")

        try:
            # Process individual file
            processed_df = preprocess_single_file(file_path)

            # Save processed file
            output_path = generate_output_path(file_path, config['output_suffix'])
            processed_df.to_csv(output_path, index=False)

            results['processed_files'].append({{
                'input_path': file_path,
                'output_path': output_path,
                'shape': processed_df.shape,
                'status': 'success'
            }})
            results['total_processed'] += 1

            print(f"   ‚úÖ Saved to: {{output_path}}")

        except Exception as e:
            error_info = {{'file': file_path, 'error': str(e)}}
            results['errors'].append(error_info)
            print(f"   ‚ùå Error: {{e}}")

    return results

def process_files_parallel(files, config):
    """Process files in parallel using ThreadPoolExecutor."""
    from concurrent.futures import ThreadPoolExecutor, as_completed

    results = {{'processed_files': [], 'total_processed': 0, 'errors': []}}
    max_workers = config.get('max_workers', 4)

    print(f"üöÄ Processing {{len(files)}} files in parallel (max {{max_workers}} workers)")

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all files for processing
        future_to_file = {{
            executor.submit(process_single_file_safe, file_path, config['output_suffix']): file_path
            for file_path in files
        }}

        # Collect results as they complete
        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                result = future.result()
                if result['status'] == 'success':
                    results['processed_files'].append(result)
                    results['total_processed'] += 1
                    print(f"‚úÖ {{Path(file_path).name}} ‚Üí {{Path(result['output_path']).name}}")
                else:
                    results['errors'].append({{'file': file_path, 'error': result['error']}})
                    print(f"‚ùå {{Path(file_path).name}}: {{result['error']}}")
            except Exception as e:
                results['errors'].append({{'file': file_path, 'error': str(e)}})
                print(f"‚ùå {{Path(file_path).name}}: {{e}}")

    return results

def process_single_file_safe(file_path, output_suffix):
    """Safely process a single file and return result info."""
    try:
        processed_df = preprocess_single_file(file_path)
        output_path = generate_output_path(file_path, output_suffix)
        processed_df.to_csv(output_path, index=False)

        return {{
            'input_path': file_path,
            'output_path': output_path,
            'shape': processed_df.shape,
            'status': 'success'
        }}
    except Exception as e:
        return {{
            'input_path': file_path,
            'status': 'error',
            'error': str(e)
        }}

def preprocess_single_file(file_path):
    """
    Apply preprocessing steps to a single file.

    Args:
        file_path (str): Path to the input file

    Returns:
        pandas.DataFrame: Preprocessed data
    """
    print(f"   üìÇ Loading {{Path(file_path).name}}...")

    # Load data based on file extension
    file_ext = Path(file_path).suffix.lower()
    if file_ext == '.csv':
        df = pd.read_csv(file_path)
    elif file_ext in ['.json', '.jsonl']:
        df = pd.read_json(file_path, lines=(file_ext == '.jsonl'))
    else:
        raise ValueError(f"Unsupported file type: {{file_ext}}")

    print(f"   üìä Loaded: {{df.shape[0]}} rows, {{df.shape[1]}} columns")
    original_shape = df.shape

    # Apply common preprocessing steps
{}"#,
        generate_batch_preprocessing_steps(&aggregated_recommendations)
    ));

    // Add utility functions
    script.push_str(
        r#"
    print(f"   ‚úÖ Preprocessing complete: {original_shape} ‚Üí {df.shape}")
    return df

def generate_output_path(input_path, suffix):
    """Generate output path for processed file."""
    path_obj = Path(input_path)
    output_filename = f"{path_obj.stem}{suffix}{path_obj.suffix}"
    return path_obj.parent / output_filename

def generate_batch_summary(results):
    """Print a comprehensive summary of batch processing results."""
    print(f"\nüìã {' Batch Processing Summary '.center(50, '=')}")
    print(f"üìä Total Files Processed: {results['total_processed']}")
    print(f"‚ùå Errors: {len(results['errors'])}")

    if results['processed_files']:
        # Calculate shape statistics
        total_input_rows = sum(info.get('original_shape', [0, 0])[0] for info in results['processed_files'] if 'original_shape' in info)
        total_output_rows = sum(info['shape'][0] for info in results['processed_files'])
        total_input_cols = sum(info.get('original_shape', [0, 0])[1] for info in results['processed_files'] if 'original_shape' in info)
        total_output_cols = sum(info['shape'][1] for info in results['processed_files'])

        print(f"üìà Data Processed:")
        print(f"   ‚îú‚îÄ Input: {total_input_rows:,} rows, {total_input_cols} columns")
        print(f"   ‚îî‚îÄ Output: {total_output_rows:,} rows, {total_output_cols} columns")

    if results['errors']:
        print(f"\n‚ö†Ô∏è  Error Details:")
        for error in results['errors']:
            print(f"   ‚ùå {Path(error['file']).name}: {error['error']}")

    print(f"\n‚úÖ Batch preprocessing completed successfully!")

def main():
    """Main function to run batch preprocessing."""
    try:
        print("üöÄ DataProfiler Batch Preprocessing Pipeline")
        print("=" * 50)

        # Run batch preprocessing
        results = preprocess_batch()

        # Display final summary
        if results['total_processed'] > 0:
            print(f"\nüíæ Successfully processed {results['total_processed']} files")
            print("üìÅ Processed files saved with '_preprocessed' suffix")

            if results['processed_files']:
                print("\nüìã Output Files:")
                for file_info in results['processed_files'][:5]:  # Show first 5
                    print(f"   ‚îú‚îÄ {Path(file_info['output_path']).name} ({file_info['shape'][0]} rows)")

                if len(results['processed_files']) > 5:
                    print(f"   ‚îî‚îÄ ... and {len(results['processed_files']) - 5} more files")

        if results['errors']:
            print(f"\n‚ö†Ô∏è  {len(results['errors'])} files failed processing - check error details above")

    except Exception as e:
        print(f"‚ùå Batch processing failed: {e}")
        raise

if __name__ == "__main__":
    main()
"#
    );

    Ok(script)
}

/// Aggregate recommendations from multiple files to find common patterns
fn aggregate_recommendations(
    ml_scores: &HashMap<PathBuf, MlReadinessScore>,
) -> Vec<(String, Vec<&MlRecommendation>)> {
    let mut recommendation_groups: HashMap<String, Vec<&MlRecommendation>> = HashMap::new();

    for score in ml_scores.values() {
        for rec in &score.recommendations {
            recommendation_groups
                .entry(rec.category.clone())
                .or_default()
                .push(rec);
        }
    }

    // Sort by frequency and priority
    let mut aggregated: Vec<_> = recommendation_groups.into_iter().collect();
    aggregated.sort_by(|a, b| {
        let freq_cmp = b.1.len().cmp(&a.1.len());
        if freq_cmp == std::cmp::Ordering::Equal {
            // If frequencies are equal, sort by highest priority in group
            let a_max_priority =
                a.1.iter()
                    .map(|r| priority_to_numeric(&r.priority))
                    .max()
                    .unwrap_or(0);
            let b_max_priority =
                b.1.iter()
                    .map(|r| priority_to_numeric(&r.priority))
                    .max()
                    .unwrap_or(0);
            b_max_priority.cmp(&a_max_priority)
        } else {
            freq_cmp
        }
    });

    aggregated
}

/// Convert priority to numeric value for sorting
fn priority_to_numeric(priority: &RecommendationPriority) -> u8 {
    match priority {
        RecommendationPriority::Critical => 4,
        RecommendationPriority::High => 3,
        RecommendationPriority::Medium => 2,
        RecommendationPriority::Low => 1,
    }
}

/// Collect unique imports from all batch files
fn collect_batch_imports(ml_scores: &HashMap<PathBuf, MlReadinessScore>) -> Vec<String> {
    let mut imports_set = HashSet::new();

    // Standard imports for batch processing
    imports_set.insert("import pandas as pd".to_string());
    imports_set.insert("import numpy as np".to_string());
    imports_set.insert("from pathlib import Path".to_string());

    // Collect imports from all recommendations
    for score in ml_scores.values() {
        for rec in &score.recommendations {
            for import in &rec.imports {
                imports_set.insert(import.clone());
            }
        }
    }

    let mut imports: Vec<String> = imports_set.into_iter().collect();
    imports.sort();
    imports
}

/// Generate common preprocessing functions for batch processing
fn generate_common_preprocessing_functions(
    aggregated_recs: &[(String, Vec<&MlRecommendation>)],
) -> String {
    let mut functions = String::new();

    for (category, recs) in aggregated_recs.iter().take(5) {
        // Limit to top 5 most common
        if let Some(sample_rec) = recs.first() {
            if let Some(code) = &sample_rec.code_snippet {
                let function_name = category.to_lowercase().replace(" ", "_").replace("-", "_");

                functions.push_str(&format!(
                    r#"
def apply_{}(df):
    """
    Apply {} preprocessing (found in {} files).
    {}
    """
    print(f"   üîß Applying {}")
    original_shape = df.shape

    {}

    print(f"      Shape change: {{original_shape}} ‚Üí {{df.shape}}")
    return df
"#,
                    function_name,
                    category,
                    recs.len(),
                    sample_rec.description,
                    category,
                    indent_batch_code(code, 4)
                ));
            }
        }
    }

    functions
}

/// Generate preprocessing steps for batch processing
fn generate_batch_preprocessing_steps(
    aggregated_recs: &[(String, Vec<&MlRecommendation>)],
) -> String {
    let mut steps = String::new();

    steps.push_str("    # Apply common preprocessing steps found across multiple files\n");

    for (category, recs) in aggregated_recs.iter().take(5) {
        if recs.first().and_then(|r| r.code_snippet.as_ref()).is_some() {
            let function_name = category.to_lowercase().replace(" ", "_").replace("-", "_");
            steps.push_str(&format!(
                "    df = apply_{}(df)  # Applied to {} files in analysis\n",
                function_name,
                recs.len()
            ));
        }
    }

    if aggregated_recs.len() > 5 {
        steps.push_str(&format!(
            "    # Note: {} additional recommendation types available\n",
            aggregated_recs.len() - 5
        ));
    }

    steps
}

/// Indent code for batch processing functions
fn indent_batch_code(code: &str, spaces: usize) -> String {
    let indent = " ".repeat(spaces);
    code.replace("\\n", "\n")
        .lines()
        .map(|line| {
            if line.trim().is_empty() {
                String::new()
            } else {
                // Remove any existing indentation and apply our own
                format!("{}{}", indent, line.trim())
            }
        })
        .collect::<Vec<_>>()
        .join("\n")
}

/// Generates a database preprocessing Python script from ML recommendations
pub fn generate_database_preprocessing_script(
    ml_score: &MlReadinessScore,
    output_path: &Path,
    connection_string: &str,
    query: &str,
) -> Result<()> {
    let script_content =
        create_database_preprocessing_script_content(ml_score, connection_string, query)?;
    fs::write(output_path, script_content)?;
    Ok(())
}

/// Creates the content of a database preprocessing Python script
fn create_database_preprocessing_script_content(
    ml_score: &MlReadinessScore,
    connection_string: &str,
    query: &str,
) -> Result<String> {
    let mut script = String::new();

    // Extract database type from connection string
    let db_type = if connection_string.starts_with("postgresql://")
        || connection_string.starts_with("postgres://")
    {
        "PostgreSQL"
    } else if connection_string.starts_with("mysql://") {
        "MySQL"
    } else if connection_string.starts_with("sqlite://") {
        "SQLite"
    } else {
        "Database"
    };

    // Add header comment
    script.push_str(&format!(
        r#"#!/usr/bin/env python3
"""
Database Preprocessing Script for ML Pipeline
Generated by DataProf Database Analysis

Database: {} Connection
Query: {}
Overall ML Readiness: {:.1}% ({})
Generated {} recommendations with actionable code snippets
"""

"#,
        db_type,
        query,
        ml_score.overall_score,
        if ml_score.overall_score >= 80.0 {
            "Ready"
        } else if ml_score.overall_score >= 60.0 {
            "Good"
        } else if ml_score.overall_score >= 40.0 {
            "Needs Work"
        } else {
            "Not Ready"
        },
        ml_score.recommendations.len()
    ));

    // Add essential imports
    let imports = vec![
        "import pandas as pd",
        "from sqlalchemy import create_engine",
    ];

    for import in &imports {
        script.push_str(&format!("{}\n", import));
    }
    script.push('\n');

    // Add database configuration
    script.push_str(&format!(
        r#"# Database configuration
DATABASE_CONFIG = {{
    'connection_string': r"{}",
    'query': r"""{}""",
    'batch_size': 10000,
    'timeout': 300,
}}

"#,
        connection_string, query
    ));

    // Add main function
    script.push_str(
        r#"def preprocess_database_data(config=None):
    """
    Preprocess database data for ML based on DataProf recommendations.

    Args:
        config (dict, optional): Database configuration

    Returns:
        pandas.DataFrame: Preprocessed data ready for ML
    """
    if config is None:
        config = DATABASE_CONFIG

    connection_string = config['connection_string']
    query = config['query']

    print("üîÑ Connecting to database...")

    # Create database engine
    engine = create_engine(connection_string)

    print(f"üìä Executing query: {query[:100]}{'...' if len(query) > 100 else ''}")

    # Load data from database
    df = pd.read_sql_query(query, engine)
    print(f"üìä Loaded data: {df.shape[0]} rows, {df.shape[1]} columns")

    original_shape = df.shape

"#,
    );

    // Add preprocessing steps from recommendations
    if !ml_score.recommendations.is_empty() {
        script.push_str("    # ========== ML READINESS RECOMMENDATIONS ==========\n");
        for (i, rec) in ml_score.recommendations.iter().enumerate() {
            script.push_str(&format!(
                "    # Step {}: {} [{}]\n",
                i + 1,
                rec.category,
                match rec.priority {
                    dataprof::analysis::RecommendationPriority::Critical => "CRITICAL",
                    dataprof::analysis::RecommendationPriority::High => "HIGH",
                    dataprof::analysis::RecommendationPriority::Medium => "MEDIUM",
                    dataprof::analysis::RecommendationPriority::Low => "LOW",
                }
            ));
            script.push_str(&format!("    # {}\n", rec.description));
            script.push_str(&format!(
                "    print(f\"üîß Step {}: {}\")\n",
                i + 1,
                rec.description
            ));

            // Add actual code if available
            if let Some(code) = &rec.code_snippet {
                script.push_str("    \n");
                // Indent the code properly
                for line in code.lines() {
                    if !line.trim().is_empty() {
                        script.push_str(&format!("    {}\n", line));
                    } else {
                        script.push('\n');
                    }
                }
                script.push('\n');
            } else {
                script.push_str("    # TODO: Implement this preprocessing step\n");
                script.push_str("    pass\n\n");
            }
        }
    }

    // Add return statement and summary
    script.push_str(
        r#"    final_shape = df.shape
    print(f"‚úÖ Database preprocessing complete!")
    print(f"üìä Final data shape: {final_shape[0]} rows, {final_shape[1]} columns")
    print(f"üîÑ Shape change: {original_shape} ‚Üí {final_shape}")

    # Close database connection
    engine.dispose()
    print("üîê Database connection closed")

    return df


def save_preprocessed_data(df, output_format='csv', output_path=None):
    """
    Save preprocessed data to various formats.

    Args:
        df (pandas.DataFrame): Preprocessed data
        output_format (str): Output format ('csv', 'parquet', 'json', 'excel')
        output_path (str, optional): Output file path

    Returns:
        str: Path to saved file
    """
    if output_path is None:
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"database_preprocessed_{timestamp}.{output_format}"

    print(f"üíæ Saving preprocessed data to: {output_path}")

    if output_format.lower() == 'csv':
        df.to_csv(output_path, index=False)
    elif output_format.lower() == 'parquet':
        df.to_parquet(output_path, index=False)
    elif output_format.lower() == 'json':
        df.to_json(output_path, orient='records', lines=True)
    elif output_format.lower() == 'excel':
        df.to_excel(output_path, index=False)
    else:
        raise ValueError(f"Unsupported output format: {output_format}")

    return output_path


def main():
    """Main function to run the database preprocessing pipeline."""
    try:
        print("üêò DataProf Database Preprocessing Pipeline")
        print("=" * 50)

        # Run preprocessing
        processed_df = preprocess_database_data()

        # Save processed data
        output_file = save_preprocessed_data(processed_df, output_format='csv')
        print(f"üíæ Saved preprocessed data to: {output_file}")

        # Display basic info about processed data
        print("\nüìã Processed Data Summary:")
        print(processed_df.info())

        if processed_df.shape[1] <= 20:  # Show describe for manageable number of columns
            print("\nüìä Statistical Summary:")
            print(processed_df.describe())

        # Optional: Save in multiple formats
        print("\nüîÑ Additional export options:")
        print("üìÑ CSV: df.to_csv('output.csv', index=False)")
        print("üóúÔ∏è  Parquet: df.to_parquet('output.parquet', index=False)")
        print("üìä Excel: df.to_excel('output.xlsx', index=False)")
        print("üîó JSON: df.to_json('output.jsonl', orient='records', lines=True)")

    except Exception as e:
        print(f"‚ùå Error during database preprocessing: {e}")
        raise


if __name__ == "__main__":
    main()
"#,
    );

    Ok(script)
}
