name: Performance Benchmarks
on:
  push:
    branches: [main, master, staging]
    paths:
      - 'src/**'
      - 'benches/**'
      - 'scripts/benchmark_comparison.py'
      - '.github/workflows/benchmarks.yml'
      - 'Cargo.toml'
      - 'Cargo.lock'
  pull_request:
    branches: [main, master, staging]
    paths:
      - 'src/**'
      - 'benches/**'
      - 'scripts/benchmark_comparison.py'
      - '.github/workflows/benchmarks.yml'
      - 'Cargo.toml'
      - 'Cargo.lock'
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Benchmark type to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - simple
          - large-scale
          - memory
          - comparison

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  # Quick benchmark build check
  benchmark-check:
    name: Benchmark Build Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: benchmark-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            benchmark-${{ runner.os }}-

      - name: Build benchmarks
        run: |
          echo "ğŸ”¨ Building all benchmarks..."
          cargo bench --no-run
          echo "âœ… Benchmark build completed"

  # Run lightweight benchmarks on PR
  pr-benchmarks:
    name: PR Performance Check
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    needs: benchmark-check
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: benchmark-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}

      - name: Run simple benchmarks
        run: |
          echo "âš¡ Running lightweight benchmarks for PR validation..."
          cargo bench --bench simple_benchmarks -- --sample-size 10 --measurement-time 10
          echo "âœ… Simple benchmarks completed"

      - name: Run memory check
        run: |
          echo "ğŸ§  Running memory efficiency check..."
          cargo bench --bench memory_benchmarks -- --sample-size 10 --measurement-time 10 "memory_pattern/1K"
          echo "âœ… Memory check completed"

  # Full benchmark suite for main/staging branches
  full-benchmarks:
    name: Full Performance Suite
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master' || github.ref == 'refs/heads/staging')
    needs: benchmark-check
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need history for trend analysis

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: benchmark-full-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}

      - name: Install Python for comparison benchmarks
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          pip install pandas numpy psutil

      - name: Create benchmark results directory
        run: mkdir -p benchmark-results

      - name: Run simple benchmarks
        run: |
          echo "ğŸƒ Running simple benchmarks..."
          cargo bench --bench simple_benchmarks > benchmark-results/simple.txt 2>&1
          echo "âœ… Simple benchmarks completed"

      - name: Run large-scale benchmarks
        run: |
          echo "ğŸ“Š Running large-scale benchmarks..."
          cargo bench --bench large_scale_benchmarks > benchmark-results/large_scale.txt 2>&1
          echo "âœ… Large-scale benchmarks completed"

      - name: Run memory benchmarks
        run: |
          echo "ğŸ§  Running memory benchmarks..."
          cargo bench --bench memory_benchmarks > benchmark-results/memory.txt 2>&1
          echo "âœ… Memory benchmarks completed"

      - name: Run external comparison benchmarks
        run: |
          echo "ğŸ”„ Running pandas comparison benchmarks..."
          chmod +x scripts/benchmark_comparison.py
          python scripts/benchmark_comparison.py 1 10 > benchmark-results/comparison.txt 2>&1 || true
          echo "âœ… Comparison benchmarks completed"

      - name: Generate benchmark report
        run: |
          echo "ğŸ“ˆ Generating benchmark report..."
          cat > benchmark-results/report.md << 'EOF'
          # DataProfiler Performance Benchmark Report

          **Branch:** `${{ github.ref_name }}`
          **Commit:** `${{ github.sha }}`
          **Date:** $(date -u)

          ## Benchmark Results

          ### Simple Benchmarks
          ```
          $(head -20 benchmark-results/simple.txt 2>/dev/null || echo "Simple benchmarks data not available")
          ```

          ### Large Scale Performance
          ```
          $(head -20 benchmark-results/large_scale.txt 2>/dev/null || echo "Large scale benchmarks data not available")
          ```

          ### Memory Usage
          ```
          $(head -20 benchmark-results/memory.txt 2>/dev/null || echo "Memory benchmarks data not available")
          ```

          ### External Tool Comparison
          ```
          $(cat benchmark-results/comparison.txt 2>/dev/null || echo "Comparison benchmarks data not available")
          ```

          ---
          ğŸ¤– Generated automatically by GitHub Actions
          EOF

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/
          retention-days: 30

      - name: Check for performance regressions
        run: |
          echo "ğŸ” Checking for performance regressions..."

          # Simple regression detection based on external comparison
          if grep -q "CLAIM FAILED" benchmark-results/comparison.txt 2>/dev/null; then
            echo "âŒ Performance regression detected!"
            echo "DataProfiler is no longer meeting the 5x faster minimum threshold"
            exit 1
          elif grep -q "CLAIM PARTIAL" benchmark-results/comparison.txt 2>/dev/null; then
            echo "âš ï¸ Performance warning: DataProfiler is 5-10x faster (below 10x claim)"
          elif grep -q "CLAIM VALIDATED" benchmark-results/comparison.txt 2>/dev/null; then
            echo "âœ… Performance claims validated: DataProfiler is â‰¥10x faster"
          else
            echo "â„¹ï¸ Performance comparison inconclusive"
          fi

          echo "âœ… Performance regression check completed"

  # Benchmark comparison on manual trigger
  manual-comparison:
    name: Manual Benchmark Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: benchmark-${{ runner.os }}-${{ hashFiles('**/Cargo.lock') }}

      - name: Install Python dependencies
        run: |
          pip install pandas numpy psutil polars

      - name: Determine benchmark type
        run: |
          BENCHMARK_TYPE="${{ github.event.inputs.benchmark_type || 'all' }}"
          echo "BENCHMARK_TYPE=$BENCHMARK_TYPE" >> $GITHUB_ENV
          echo "Selected benchmark type: $BENCHMARK_TYPE"

      - name: Run selected benchmarks
        run: |
          case "$BENCHMARK_TYPE" in
            "simple")
              cargo bench --bench simple_benchmarks
              ;;
            "large-scale")
              cargo bench --bench large_scale_benchmarks -- --sample-size 10
              ;;
            "memory")
              cargo bench --bench memory_benchmarks -- --sample-size 10
              ;;
            "comparison")
              python scripts/benchmark_comparison.py 1 5 25
              ;;
            "all"|*)
              cargo bench --bench simple_benchmarks -- --sample-size 10
              cargo bench --bench memory_benchmarks -- --sample-size 10 "memory_pattern/1K"
              python scripts/benchmark_comparison.py 1 10
              ;;
          esac

      - name: Upload manual benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: manual-benchmark-${{ env.BENCHMARK_TYPE }}-${{ github.sha }}
          path: |
            target/criterion/
            benchmark_comparison_results.json
          retention-days: 7

  # Performance trend analysis with GitHub Pages
  trend-analysis:
    name: Performance Trend Analysis
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')
    needs: full-benchmarks
    permissions:
      contents: read
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/

      - name: Setup performance data for GitHub Pages
        run: |
          # Create performance data structure
          mkdir -p performance-site/{data,assets}

          # Store current benchmark with timestamp
          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          mkdir -p performance-site/data/$TIMESTAMP
          cp -r benchmark-results/* performance-site/data/$TIMESTAMP/

          # Create latest.json with metadata
          cat > performance-site/data/latest.json << EOF
          {
            "timestamp": "$(date -u --iso-8601)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "run_id": "${{ github.run_id }}",
            "results_path": "$TIMESTAMP"
          }
          EOF

          # Create simple HTML dashboard
          cat > performance-site/index.html << 'EOF'
          <!DOCTYPE html>
          <html>
          <head>
            <title>DataProfiler Performance Dashboard</title>
            <meta charset="utf-8">
            <style>
              body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; margin: 40px; }
              .header { border-bottom: 1px solid #eee; padding-bottom: 20px; margin-bottom: 30px; }
              .metric { background: #f8f9fa; padding: 20px; margin: 10px 0; border-radius: 6px; }
              .success { border-left: 4px solid #28a745; }
              .warning { border-left: 4px solid #ffc107; }
              .error { border-left: 4px solid #dc3545; }
              pre { background: #f1f3f4; padding: 15px; overflow-x: auto; border-radius: 4px; }
              .timestamp { color: #6c757d; font-size: 0.9em; }
            </style>
          </head>
          <body>
            <div class="header">
              <h1>ğŸš€ DataProfiler Performance Dashboard</h1>
              <p class="timestamp">Last updated: $(date -u)</p>
              <p>Commit: <code>${{ github.sha }}</code> | Branch: <code>${{ github.ref_name }}</code></p>
            </div>

            <div class="metric success">
              <h3>ğŸ“Š Latest Benchmark Results</h3>
              <p>Performance benchmarks completed successfully.</p>
              <a href="data/$TIMESTAMP/report.md">ğŸ“‹ Full Report</a> |
              <a href="data/$TIMESTAMP/comparison.txt">ğŸ”„ pandas Comparison</a>
            </div>

            <div class="metric">
              <h3>ğŸ—ï¸ Build Information</h3>
              <ul>
                <li><strong>Run ID:</strong> ${{ github.run_id }}</li>
                <li><strong>Workflow:</strong> Performance Benchmarks</li>
                <li><strong>Trigger:</strong> ${{ github.event_name }}</li>
              </ul>
            </div>

            <div class="metric">
              <h3>ğŸ“ˆ Performance Claims Validation</h3>
              <p>Automated validation of "10x faster than pandas" claims through CI benchmarks.</p>
              <pre id="comparison-results">Loading comparison results...</pre>
            </div>

            <script>
              // Load comparison results dynamically
              fetch('data/$TIMESTAMP/comparison.txt')
                .then(response => response.text())
                .then(data => {
                  document.getElementById('comparison-results').textContent = data;
                })
                .catch(err => {
                  document.getElementById('comparison-results').textContent = 'Comparison results not available';
                });
            </script>
          </body>
          </html>
          EOF

      - name: Store performance data as artifact
        uses: actions/upload-artifact@v4
        with:
          name: performance-history-${{ github.sha }}
          path: performance-site/
          retention-days: 90

      - name: Deploy to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./performance-site

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
