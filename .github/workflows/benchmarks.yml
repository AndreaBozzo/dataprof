name: Comprehensive Performance Benchmarks
on:
  # Only on main/master pushes for full benchmarks
  push:
    branches: [main, master]
    paths:
      - 'src/**'
      - 'benches/**'
      - 'scripts/benchmark_comparison.py'
      - '.github/workflows/benchmarks.yml'
      - 'Cargo.toml'
      - 'Cargo.lock'

  # Scheduled full benchmark runs
  schedule:
    - cron: '0 2 * * 1,4'  # Monday and Thursday at 2 AM UTC

  # Manual dispatch for on-demand benchmarks
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options: [all, unified, domain, statistical, comparison]

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  # Build check with parallel cache warming
  build-check:
    name: Build Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-prefix: benchmark

      - name: Build benchmarks
        run: |
          echo "ðŸ”¨ Building all benchmarks..."
          timeout 600 cargo bench --no-run
          echo "âœ… Benchmark build completed"

  # Full benchmark suite for main/master branches
  comprehensive-benchmarks:
    name: Full Performance Suite
    runs-on: ubuntu-latest
    needs: build-check
    timeout-minutes: 90
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
        with:
          fetch-depth: 0  # Need history for trend analysis

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-prefix: benchmark

      - name: Setup Python dependencies
        uses: ./.github/actions/setup-python-deps

      - name: Create benchmark results directory
        run: mkdir -p benchmark-results

      - name: Run unified benchmarks
        uses: ./.github/actions/benchmark-runner
        with:
          benchmark-type: unified
          mode: full
          timeout: 25

      - name: Run domain benchmarks
        uses: ./.github/actions/benchmark-runner
        with:
          benchmark-type: domain
          mode: full
          timeout: 25

      - name: Run statistical benchmarks
        uses: ./.github/actions/benchmark-runner
        with:
          benchmark-type: statistical
          mode: full
          timeout: 25

      - name: Run external comparison benchmarks
        run: |
          echo "ðŸ”„ Running external tool comparison..."
          if timeout 900 python scripts/benchmark_comparison.py 1 5 10 > benchmark-results/comparison.txt 2>&1; then
            echo "âœ… Comparison benchmarks completed successfully"
          else
            echo "âš ï¸ Comparison benchmarks timed out or failed - generating fallback report"
            echo "Comparison benchmarks were skipped due to timeout or tool unavailability." > benchmark-results/comparison.txt
            echo "This is normal in CI environments where external tools may not be available." >> benchmark-results/comparison.txt
          fi

      - name: Performance regression analysis
        run: |
          echo "ðŸ” Running performance regression analysis..."

          # Create baseline if missing
          if [[ ! -f "benchmark-results/performance_baseline.json" ]]; then
            echo "âš ï¸ Creating initial baseline..."
            echo '{"version":"1.0","benchmarks":[],"statistics":{}}' > benchmark-results/performance_baseline.json
          fi

          # Run regression analysis for each benchmark type
          for benchmark_type in unified domain; do
            if [[ -f "benchmark-results/${benchmark_type}_benchmark_results.json" ]]; then
              python scripts/performance_regression_check.py \
                "benchmark-results/${benchmark_type}_benchmark_results.json" \
                --baseline "benchmark-results/${benchmark_type}_baseline.json" \
                --update-baseline > "benchmark-results/${benchmark_type}_regression_analysis.txt" 2>&1 || \
              echo "Regression analysis for ${benchmark_type} completed with warnings"
            fi
          done

          echo "âœ… Regression analysis completed"

      - name: Generate comprehensive report
        run: |
          echo "ðŸ“ˆ Generating benchmark report..."

          cat > benchmark-results/report.md << 'EOF'
          # DataProfiler Performance Benchmark Report

          **Branch:** `${{ github.ref_name }}`
          **Commit:** `${{ github.sha }}`
          **Date:** $(date -u)

          ## ðŸš€ Benchmark Results

          ### Unified Performance Suite
          ```
          $(head -30 benchmark-results/unified.txt 2>/dev/null || echo "Unified benchmarks data not available")
          ```

          ### Domain-Specific Performance
          ```
          $(head -30 benchmark-results/domain.txt 2>/dev/null || echo "Domain benchmarks data not available")
          ```

          ### Statistical Rigor Framework
          ```
          $(head -30 benchmark-results/statistical.txt 2>/dev/null || echo "Statistical benchmarks data not available")
          ```

          ### External Tool Comparison
          ```
          $(cat benchmark-results/comparison.txt 2>/dev/null || echo "Comparison benchmarks data not available")
          ```

          ## ðŸ“Š Regression Analysis

          ### Unified Benchmarks
          ```
          $(cat benchmark-results/unified_regression_analysis.txt 2>/dev/null || echo "Analysis not available")
          ```

          ### Domain Benchmarks
          ```
          $(cat benchmark-results/domain_regression_analysis.txt 2>/dev/null || echo "Analysis not available")
          ```

          ---
          ðŸ¤– Generated automatically by GitHub Actions | Run ID: ${{ github.run_id }}
          EOF

          echo "âœ… Report generated"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/
          retention-days: 30

      - name: Performance summary
        run: |
          echo "ðŸ“Š Performance Results Summary"
          echo "================================="

          if [[ -f "benchmark-results/comparison.txt" ]]; then
            # Extract performance assessment
            if grep -q "ðŸ† OUTSTANDING\|ðŸ¥‡ EXCELLENT" benchmark-results/comparison.txt; then
              echo "ðŸ† EXCELLENT: Outstanding performance achieved!"
            elif grep -q "ðŸ¥ˆ COMPETITIVE\|ðŸ’¾ MEMORY CHAMPION" benchmark-results/comparison.txt; then
              echo "ðŸ¥ˆ COMPETITIVE: Good performance with excellent efficiency!"
            else
              echo "âœ… Performance validation completed"
            fi

            echo ""
            echo "ðŸŽ¯ Key Metrics:"
            grep -E "(Speed:|Memory:)" benchmark-results/comparison.txt 2>/dev/null | head -5 || echo "Metrics in detailed report"
          fi

          echo ""
          echo "âœ… Comprehensive benchmark analysis completed"

  # Manual benchmark execution
  manual-benchmarks:
    name: Manual Benchmark Execution
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch'
    timeout-minutes: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Setup Rust
        uses: ./.github/actions/setup-rust
        with:
          cache-prefix: benchmark

      - name: Setup Python dependencies
        if: github.event.inputs.benchmark_type == 'comparison' || github.event.inputs.benchmark_type == 'all'
        uses: ./.github/actions/setup-python-deps

      - name: Execute selected benchmarks
        uses: ./.github/actions/benchmark-runner
        with:
          benchmark-type: ${{ github.event.inputs.benchmark_type }}
          mode: full
          timeout: 45

      - name: Run external comparison
        if: github.event.inputs.benchmark_type == 'comparison' || github.event.inputs.benchmark_type == 'all'
        run: |
          echo "ðŸ”„ Running external tool comparison..."
          python scripts/benchmark_comparison.py 1 10
          echo "âœ… External comparison completed"

      - name: Upload manual results
        uses: actions/upload-artifact@v4
        with:
          name: manual-benchmark-${{ github.event.inputs.benchmark_type }}-${{ github.sha }}
          path: |
            target/criterion/
            benchmark-results/
          retention-days: 7

  # Performance trend analysis with GitHub Pages
  performance-dashboard:
    name: Update Performance Dashboard
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'
    needs: comprehensive-benchmarks
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: benchmark-results/

      - name: Generate GitHub Pages site
        run: |
          # Create performance site structure
          mkdir -p performance-site/{data,assets}

          TIMESTAMP=$(date -u +%Y%m%d_%H%M%S)
          mkdir -p performance-site/data/$TIMESTAMP
          cp -r benchmark-results/* performance-site/data/$TIMESTAMP/

          # Create metadata
          cat > performance-site/data/latest.json << EOF
          {
            "timestamp": "$(date -u --iso-8601)",
            "commit": "${{ github.sha }}",
            "branch": "${{ github.ref_name }}",
            "run_id": "${{ github.run_id }}",
            "results_path": "$TIMESTAMP",
            "workflow_url": "https://github.com/AndreaBozzo/dataprof/actions/runs/${{ github.run_id }}",
            "status": "completed"
          }
          EOF

          # Generate dashboard from template
          cp .github/templates/dashboard.html performance-site/index.html

          # Replace placeholders
          sed -i "s/TIMESTAMP_PLACEHOLDER/$TIMESTAMP/g" performance-site/index.html
          sed -i "s/COMMIT_PLACEHOLDER/${{ github.sha }}/g" performance-site/index.html
          sed -i "s/BRANCH_PLACEHOLDER/${{ github.ref_name }}/g" performance-site/index.html
          sed -i "s|WORKFLOW_URL_PLACEHOLDER|https://github.com/AndreaBozzo/dataprof/actions/runs/${{ github.run_id }}|g" performance-site/index.html

          echo "âœ… Performance dashboard generated"

      - name: Deploy to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./performance-site

      - name: Deploy GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4