{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataProf Working Demo - Data Profiling & Quality Analysis\n",
    "\n",
    "This notebook demonstrates the **working** functionality of the `dataprof` library for data profiling and quality assessment.\n",
    "\n",
    "**Version:** 0.4.1 (confirmed working)  \n",
    "**Date:** 2025-01-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š DataProf Working Demo\n",
      "==================================================\n",
      "Dataprof version: Unknown\n",
      "Pandas version: 2.3.2\n",
      "\n",
      "Available dataprof functions:\n",
      "  âœ“ PyBatchResult\n",
      "  âœ“ PyColumnProfile\n",
      "  âœ“ PyQualityIssue\n",
      "  âœ“ PyQualityReport\n",
      "  âœ“ analyze_csv_file\n",
      "  âœ“ analyze_csv_with_quality\n",
      "  âœ“ analyze_json_file\n",
      "  âœ“ batch_analyze_directory\n",
      "  âœ“ batch_analyze_glob\n",
      "\n",
      "âœ… Setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dataprof as dp\n",
    "import os\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"ğŸ“Š DataProf Working Demo\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataprof version: {getattr(dp, '__version__', 'Unknown')}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print()\n",
    "print(\"Available dataprof functions:\")\n",
    "functions = [f for f in dir(dp) if not f.startswith('_') and callable(getattr(dp, f))]\n",
    "for func in functions:\n",
    "    print(f\"  âœ“ {func}\")\n",
    "print()\n",
    "print(\"âœ… Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Sample Dataset Created\n",
      "==============================\n",
      "Shape: (100, 6)\n",
      "Columns: ['customer_id', 'age', 'income', 'city', 'purchase_amount', 'satisfaction_score']\n",
      "\n",
      "First 5 rows:\n",
      "   customer_id   age   income         city  purchase_amount  \\\n",
      "0            1  25.0  35000.0     New York           114.90   \n",
      "1            2  32.0  52000.0  Los Angeles            95.85   \n",
      "2            3  45.0  78000.0      Chicago           119.43   \n",
      "3            4   NaN  45000.0      Houston           145.69   \n",
      "4            5  28.0      NaN      Phoenix            92.98   \n",
      "\n",
      "   satisfaction_score  \n",
      "0                   1  \n",
      "1                   5  \n",
      "2                   1  \n",
      "3                   3  \n",
      "4                   2  \n",
      "\n",
      "Missing values per column:\n",
      "  customer_id: 0 (0.0%)\n",
      "  age: 11 (11.0%)\n",
      "  income: 11 (11.0%)\n",
      "  city: 11 (11.0%)\n",
      "  purchase_amount: 0 (0.0%)\n",
      "  satisfaction_score: 0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create a realistic dataset with various data quality issues\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "data = {\n",
    "    'customer_id': range(1, 101),\n",
    "    'age': [25, 32, 45, None, 28, 31, 67, 23, 29, 41] * 10,\n",
    "    'income': [35000, 52000, 78000, 45000, None, 28000, 95000, 31000, 48000, 67000] * 10,\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', None, 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas'] * 10,\n",
    "    'purchase_amount': np.random.normal(100, 30, 100).round(2),\n",
    "    'satisfaction_score': np.random.randint(1, 6, 100)\n",
    "}\n",
    "\n",
    "# Introduce some missing values\n",
    "data['age'][15] = None\n",
    "data['age'][33] = None\n",
    "data['income'][22] = None\n",
    "data['city'][8] = None\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"ğŸ“‹ Sample Dataset Created\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print()\n",
    "print(\"First 5 rows:\")\n",
    "print(df.head())\n",
    "print()\n",
    "print(\"Missing values per column:\")\n",
    "missing_data = df.isnull().sum()\n",
    "for col, missing in missing_data.items():\n",
    "    if missing > 0:\n",
    "        print(f\"  {col}: {missing} ({missing/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"  {col}: 0 (0.0%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Dataset saved to: customer_data.csv\n",
      "\n",
      "ğŸ” Basic Data Profiling\n",
      "==============================\n",
      "âœ… Analysis completed: 6 column profiles generated\n",
      "\n",
      "ğŸ“Š Column Profile Summary:\n",
      "\n",
      "Column 1 Profile:\n",
      "  data_type: integer\n",
      "  name: customer_id\n",
      "  null_count: 0\n",
      "  null_percentage: 0.0\n",
      "  total_count: 100\n",
      "  unique_count: 100\n",
      "  uniqueness_ratio: 1.0\n",
      "\n",
      "Column 2 Profile:\n",
      "  data_type: float\n",
      "  name: age\n",
      "  null_count: 11\n",
      "  null_percentage: 11.0\n",
      "  total_count: 100\n",
      "  unique_count: 10\n",
      "  uniqueness_ratio: 0.1\n",
      "\n",
      "Column 3 Profile:\n",
      "  data_type: float\n",
      "  name: purchase_amount\n",
      "  null_count: 0\n",
      "  null_percentage: 0.0\n",
      "  total_count: 100\n",
      "  unique_count: 99\n",
      "  uniqueness_ratio: 0.99\n",
      "\n",
      "Column 4 Profile:\n",
      "  data_type: string\n",
      "  name: city\n",
      "  null_count: 11\n",
      "  null_percentage: 11.0\n",
      "  total_count: 100\n",
      "  unique_count: 10\n",
      "  uniqueness_ratio: 0.1\n",
      "\n",
      "Column 5 Profile:\n",
      "  data_type: float\n",
      "  name: income\n",
      "  null_count: 11\n",
      "  null_percentage: 11.0\n",
      "  total_count: 100\n",
      "  unique_count: 10\n",
      "  uniqueness_ratio: 0.1\n",
      "\n",
      "Column 6 Profile:\n",
      "  data_type: integer\n",
      "  name: satisfaction_score\n",
      "  null_count: 0\n",
      "  null_percentage: 0.0\n",
      "  total_count: 100\n",
      "  unique_count: 5\n",
      "  uniqueness_ratio: 0.05\n",
      "\n",
      "ğŸ§¹ Cleaned up customer_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save dataset for analysis\n",
    "csv_file = \"customer_data.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"ğŸ’¾ Dataset saved to: {csv_file}\")\n",
    "\n",
    "# Perform basic analysis\n",
    "print(\"\\nğŸ” Basic Data Profiling\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "try:\n",
    "    profiles = dp.analyze_csv_file(csv_file)\n",
    "    print(f\"âœ… Analysis completed: {len(profiles)} column profiles generated\")\n",
    "    \n",
    "    print(\"\\nğŸ“Š Column Profile Summary:\")\n",
    "    for i, profile in enumerate(profiles):\n",
    "        print(f\"\\nColumn {i+1} Profile:\")\n",
    "        \n",
    "        # Get available attributes\n",
    "        attrs = [attr for attr in dir(profile) if not attr.startswith('_') and not callable(getattr(profile, attr))]\n",
    "        \n",
    "        for attr in attrs[:8]:  # Show first 8 attributes\n",
    "            try:\n",
    "                value = getattr(profile, attr)\n",
    "                print(f\"  {attr}: {value}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  {attr}: Error - {e}\")\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nğŸ§¹ Cleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Data Quality Assessment\n",
      "===================================\n",
      "ğŸ“ˆ Quality Score: 50.0/100\n",
      "ğŸ“Š Total Rows: 100\n",
      "ğŸ“Š Total Columns: 6\n",
      "â±ï¸  Scan Time: 533 ms\n",
      "âš ï¸  Issues Found: 7\n",
      "\n",
      "ğŸš¨ Quality Issues Detected:\n",
      "  1. ğŸŸ¢ 11 null values (11%) in column 'age'\n",
      "     Column: age | Severity: medium\n",
      "  2. ğŸŸ¢ 90 duplicate values in column 'age'\n",
      "     Column: age | Severity: low\n",
      "  3. ğŸŸ¢ 11 null values (11%) in column 'income'\n",
      "     Column: income | Severity: medium\n",
      "  4. ğŸŸ¢ 90 duplicate values in column 'income'\n",
      "     Column: income | Severity: low\n",
      "  5. ğŸŸ¢ 11 null values (11%) in column 'city'\n",
      "     Column: city | Severity: medium\n",
      "  6. ğŸŸ¢ 90 duplicate values in column 'city'\n",
      "     Column: city | Severity: low\n",
      "  7. ğŸŸ¢ 95 duplicate values in column 'satisfaction_score'\n",
      "     Column: satisfaction_score | Severity: low\n",
      "\n",
      "ğŸ“‹ Quality Report Details:\n",
      "  column_profiles: [<builtins.PyColumnProfile object at 0x000002CEBB7FC4B0>, <builtins.PyColumnProfile object at 0x000002CEBB7FE330>, <builtins.PyColumnProfile object at 0x000002CEBB7FE430>, <builtins.PyColumnProfile object at 0x000002CEBB7FE3B0>, <builtins.PyColumnProfile object at 0x000002CEBB7FE4B0>, <builtins.PyColumnProfile object at 0x000002CEBB7FE530>]\n",
      "  file_path: customer_data_quality.csv\n",
      "  issues: [<builtins.PyQualityIssue object at 0x000002CEB1A6DF70>, <builtins.PyQualityIssue object at 0x000002CEBB812150>, <builtins.PyQualityIssue object at 0x000002CEBB8121F0>, <builtins.PyQualityIssue object at 0x000002CEBB812290>, <builtins.PyQualityIssue object at 0x000002CEBB812330>, <builtins.PyQualityIssue object at 0x000002CEBB8123D0>, <builtins.PyQualityIssue object at 0x000002CEBB812510>]\n",
      "  rows_scanned: 100\n",
      "  sampling_ratio: 1.0\n",
      "  scan_time_ms: 533\n",
      "  total_columns: 6\n",
      "  total_rows: 100\n",
      "\n",
      "ğŸ§¹ Cleaned up customer_data_quality.csv\n"
     ]
    }
   ],
   "source": [
    "# Quality analysis\n",
    "csv_file = \"customer_data_quality.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"ğŸ¯ Data Quality Assessment\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "try:\n",
    "    quality_report = dp.analyze_csv_with_quality(csv_file)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Quality Score: {quality_report.quality_score()}/100\")\n",
    "    print(f\"ğŸ“Š Total Rows: {quality_report.total_rows:,}\")\n",
    "    print(f\"ğŸ“Š Total Columns: {quality_report.total_columns}\")\n",
    "    print(f\"â±ï¸  Scan Time: {quality_report.scan_time_ms} ms\")\n",
    "    print(f\"âš ï¸  Issues Found: {len(quality_report.issues)}\")\n",
    "    \n",
    "    if quality_report.issues:\n",
    "        print(\"\\nğŸš¨ Quality Issues Detected:\")\n",
    "        for i, issue in enumerate(quality_report.issues, 1):\n",
    "            severity_icon = \"ğŸ”´\" if issue.severity == \"High\" else \"ğŸŸ¡\" if issue.severity == \"Medium\" else \"ğŸŸ¢\"\n",
    "            print(f\"  {i}. {severity_icon} {issue.description}\")\n",
    "            print(f\"     Column: {issue.column} | Severity: {issue.severity}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… No quality issues detected!\")\n",
    "        \n",
    "    # Additional quality metrics\n",
    "    print(\"\\nğŸ“‹ Quality Report Details:\")\n",
    "    attrs = [attr for attr in dir(quality_report) if not attr.startswith('_') and not callable(getattr(quality_report, attr))]\n",
    "    for attr in attrs:\n",
    "        try:\n",
    "            value = getattr(quality_report, attr)\n",
    "            print(f\"  {attr}: {value}\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during quality analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nğŸ§¹ Cleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Batch Processing Demo\n",
      "==============================\n",
      "ğŸ“„ Created: batch_sales_q1.csv\n",
      "ğŸ“„ Created: batch_sales_q2.csv\n",
      "ğŸ“„ Created: batch_employee_data.csv\n",
      "\n",
      "ğŸ”„ Running batch analysis...\n",
      "âœ… Batch analysis completed!\n",
      "ğŸ“Š Result type: <class 'builtins.PyBatchResult'>\n",
      "\n",
      "ğŸ“‹ Batch Result Attributes:\n",
      "  average_quality_score: 96.94444444444446\n",
      "  failed_files: 0\n",
      "  processed_files: 3\n",
      "  total_duration_secs: 0.4270756\n",
      "  total_quality_issues: 6\n",
      "\n",
      "ğŸ§¹ Cleaned up 3 batch test files\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“¦ Batch Processing Demo\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create multiple test datasets\n",
    "test_files = []\n",
    "datasets = {\n",
    "    'sales_q1': {\n",
    "        'month': ['Jan', 'Feb', 'Mar'] * 10,\n",
    "        'sales': np.random.normal(50000, 10000, 30).round(2),\n",
    "        'region': ['North', 'South', 'East', 'West'] * 7 + ['North', 'South']\n",
    "    },\n",
    "    'sales_q2': {\n",
    "        'month': ['Apr', 'May', 'Jun'] * 8,\n",
    "        'sales': np.random.normal(55000, 12000, 24).round(2),\n",
    "        'region': ['North', 'South', 'East', 'West'] * 6\n",
    "    },\n",
    "    'employee_data': {\n",
    "        'employee_id': range(1, 21),\n",
    "        'department': ['IT', 'HR', 'Finance', 'Marketing'] * 5,\n",
    "        'salary': np.random.normal(70000, 15000, 20).round(2),\n",
    "        'years_experience': np.random.randint(1, 16, 20)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create files\n",
    "for name, data in datasets.items():\n",
    "    filename = f\"batch_{name}.csv\"\n",
    "    pd.DataFrame(data).to_csv(filename, index=False)\n",
    "    test_files.append(filename)\n",
    "    print(f\"ğŸ“„ Created: {filename}\")\n",
    "\n",
    "try:\n",
    "    # Run batch analysis\n",
    "    print(\"\\nğŸ”„ Running batch analysis...\")\n",
    "    batch_result = dp.batch_analyze_glob(\"batch_*.csv\")\n",
    "    \n",
    "    print(f\"âœ… Batch analysis completed!\")\n",
    "    print(f\"ğŸ“Š Result type: {type(batch_result)}\")\n",
    "    \n",
    "    # Explore batch result attributes\n",
    "    print(\"\\nğŸ“‹ Batch Result Attributes:\")\n",
    "    attrs = [attr for attr in dir(batch_result) if not attr.startswith('_') and not callable(getattr(batch_result, attr))]\n",
    "    for attr in attrs:\n",
    "        try:\n",
    "            value = getattr(batch_result, attr)\n",
    "            print(f\"  {attr}: {value}\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error during batch analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "for filename in test_files:\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "print(f\"\\nğŸ§¹ Cleaned up {len(test_files)} batch test files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ DataProf Working Demo Complete!\n",
      "=============================================\n",
      "\n",
      "âœ… Successfully Demonstrated:\n",
      "   ğŸ“Š Basic data profiling with analyze_csv_file()\n",
      "   ğŸ¯ Quality assessment with analyze_csv_with_quality()\n",
      "   ğŸ“¦ Batch processing with batch_analyze_glob()\n",
      "   ğŸ” Column profiling and statistics\n",
      "   âš ï¸  Quality issue detection\n",
      "   ğŸ“ˆ Quality scoring\n",
      "\n",
      "ğŸ› ï¸  DataProf Functions Used (v0.4.1):\n",
      "   âœ“ analyze_csv_file() - Basic CSV analysis\n",
      "   âœ“ analyze_csv_with_quality() - Quality assessment\n",
      "   âœ“ batch_analyze_glob() - Batch file processing\n",
      "   âœ“ PyColumnProfile - Column statistics object\n",
      "   âœ“ PyQualityReport - Quality assessment object\n",
      "   âœ“ PyBatchResult - Batch processing result object\n",
      "\n",
      "ğŸš€ Next Steps:\n",
      "   1. Explore analyze_json_file() for JSON data\n",
      "   2. Try batch_analyze_directory() for folder processing\n",
      "   3. Integrate dataprof into your data pipelines\n",
      "   4. Set up automated quality monitoring\n",
      "   5. Create custom quality thresholds\n",
      "   6. Export quality reports for stakeholders\n",
      "\n",
      "ğŸ“š Documentation: https://github.com/AndreaBozzo/dataprof\n",
      "ğŸ› Issues: https://github.com/AndreaBozzo/dataprof/issues\n",
      "\n",
      "==================================================\n",
      "    DataProf Demo - Ready for Production Use! ğŸš€\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ‰ DataProf Working Demo Complete!\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\\nâœ… Successfully Demonstrated:\")\n",
    "print(\"   ğŸ“Š Basic data profiling with analyze_csv_file()\")\n",
    "print(\"   ğŸ¯ Quality assessment with analyze_csv_with_quality()\")\n",
    "print(\"   ğŸ“¦ Batch processing with batch_analyze_glob()\")\n",
    "print(\"   ğŸ” Column profiling and statistics\")\n",
    "print(\"   âš ï¸  Quality issue detection\")\n",
    "print(\"   ğŸ“ˆ Quality scoring\")\n",
    "\n",
    "print(\"\\nğŸ› ï¸  DataProf Functions Used (v0.4.1):\")\n",
    "working_functions = [\n",
    "    'analyze_csv_file() - Basic CSV analysis',\n",
    "    'analyze_csv_with_quality() - Quality assessment',\n",
    "    'batch_analyze_glob() - Batch file processing',\n",
    "    'PyColumnProfile - Column statistics object',\n",
    "    'PyQualityReport - Quality assessment object',\n",
    "    'PyBatchResult - Batch processing result object'\n",
    "]\n",
    "\n",
    "for func in working_functions:\n",
    "    print(f\"   âœ“ {func}\")\n",
    "\n",
    "print(\"\\nğŸš€ Next Steps:\")\n",
    "print(\"   1. Explore analyze_json_file() for JSON data\")\n",
    "print(\"   2. Try batch_analyze_directory() for folder processing\")\n",
    "print(\"   3. Integrate dataprof into your data pipelines\")\n",
    "print(\"   4. Set up automated quality monitoring\")\n",
    "print(\"   5. Create custom quality thresholds\")\n",
    "print(\"   6. Export quality reports for stakeholders\")\n",
    "\n",
    "print(\"\\nğŸ“š Documentation: https://github.com/AndreaBozzo/dataprof\")\n",
    "print(\"ğŸ› Issues: https://github.com/AndreaBozzo/dataprof/issues\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"    DataProf Demo - Ready for Production Use! ğŸš€\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-short",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
