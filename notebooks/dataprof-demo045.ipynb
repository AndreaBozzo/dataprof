{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dataprof v0.4.5 for Data Engineering & ML Workflows\n",
    "\n",
    "This notebook provides a comprehensive hands-on introduction to the [`dataprof`](https://github.com/AndreaBozzo/dataprof) library v0.4.5.  \n",
    "The goal is to demonstrate how it can be used for **data profiling**, **quality checks**, **ML readiness assessment**, and as a helper in **ML pipelines**.\n",
    "\n",
    "## üÜï New in v0.4.5:\n",
    "- **ML Readiness Assessment System** with comprehensive scoring\n",
    "- **Enhanced Pandas Integration** with DataFrame outputs  \n",
    "- **Context Managers** for resource management\n",
    "- **Security enhancements** and comprehensive fixes\n",
    "- **Python Logging Integration** with configurable levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# %pip install dataprof pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataprof version: 0.4.5\n",
      "Available functions: ['PyBatchAnalyzer', 'PyBatchResult', 'PyColumnProfile', 'PyCsvProcessor', 'PyFeatureAnalysis', 'PyMlAnalyzer', 'PyMlBlockingIssue', 'PyMlReadinessScore', 'PyMlRecommendation', 'PyPreprocessingSuggestion', 'PyQualityIssue', 'PyQualityReport', 'analyze_csv_dataframe', 'analyze_csv_file', 'analyze_csv_for_ml', 'analyze_csv_with_logging', 'analyze_csv_with_quality', 'analyze_json_file', 'batch_analyze_directory', 'batch_analyze_glob', 'configure_logging', 'feature_analysis_dataframe', 'get_logger', 'log_debug', 'log_error', 'log_info', 'log_warning', 'ml_readiness_score', 'ml_readiness_score_with_logging']\n",
      "\n",
      "=== Configuring Python Logging (New in v0.4.5) ===\n",
      "‚úÖ Python logging configured successfully!\n",
      "Setup completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dataprof as dp\n",
    "import os\n",
    "\n",
    "print(f\"Dataprof version: {getattr(dp, '__version__', 'Unknown')}\")\n",
    "print(\"Available functions:\", [f for f in dir(dp) if not f.startswith('_') and callable(getattr(dp, f))])\n",
    "\n",
    "# üÜï NEW in v0.4.5: Configure logging\n",
    "print(\"\\n=== Configuring Python Logging (New in v0.4.5) ===\")\n",
    "try:\n",
    "    dp.configure_logging(level=\"INFO\")\n",
    "    print(\"‚úÖ Python logging configured successfully!\")\n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  Logging configuration not available (requires v0.4.5+)\")\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset created:\n",
      "    age   income gender     city\n",
      "0  25.0  30000.0      M      NYC\n",
      "1  32.0  50000.0      F       LA\n",
      "2  40.0  70000.0      M  Chicago\n",
      "3   NaN  45000.0      F      NYC\n",
      "4  18.0      NaN      F   Boston\n",
      "5  22.0  22000.0   None       LA\n",
      "6  45.0  80000.0      M  Chicago\n",
      "7  33.0  55000.0      F      NYC\n",
      "\n",
      "Dataset shape: (8, 4)\n",
      "Missing values per column:\n",
      "age       1\n",
      "income    1\n",
      "gender    1\n",
      "city      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a sample dataset with some data quality issues\n",
    "data = {\n",
    "    \"age\": [25, 32, 40, None, 18, 22, 45, 33],\n",
    "    \"income\": [30000, 50000, 70000, 45000, None, 22000, 80000, 55000],\n",
    "    \"gender\": [\"M\", \"F\", \"M\", \"F\", \"F\", None, \"M\", \"F\"],\n",
    "    \"city\": [\"NYC\", \"LA\", \"Chicago\", \"NYC\", \"Boston\", \"LA\", \"Chicago\", \"NYC\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample dataset created:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Missing values per column:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Pandas Integration (New in v0.4.5) ===\n",
      "Enhanced ML dataset created:\n",
      "    age   income  experience_years gender     city  target\n",
      "0  25.0  30000.0               2.0      M      NYC       0\n",
      "1  32.0  50000.0               8.0      F       LA       1\n",
      "2  40.0  70000.0              15.0      M  Chicago       1\n",
      "3   NaN  45000.0               NaN      F      NYC       0\n",
      "4  18.0      NaN               0.0      F   Boston       0\n",
      "5  22.0  22000.0               1.0   None       LA       0\n",
      "6  45.0  80000.0              20.0      M  Chicago       1\n",
      "7  33.0  55000.0              10.0      F      NYC       1\n",
      "8  28.0  42000.0               5.0      M   Boston       0\n",
      "9  35.0  65000.0              12.0      F       LA       1\n",
      "\n",
      "Dataset shape: (10, 6)\n",
      "Target distribution:\n",
      "target\n",
      "0    5\n",
      "1    5\n",
      "Name: count, dtype: int64\n",
      "Dataset saved to ml_sample_data.csv\n",
      "\n",
      "=== Pandas DataFrame Integration (New in v0.4.5) ===\n",
      "Profiles DataFrame shape: (6, 7)\n",
      "Profiles DataFrame columns: ['uniqueness_ratio', 'column_name', 'data_type', 'null_count', 'total_count', 'null_percentage', 'unique_count']\n",
      "\n",
      "First few rows of profiles:\n",
      "   uniqueness_ratio       column_name data_type  null_count  total_count  \\\n",
      "0               0.4              city    string           0           10   \n",
      "1               1.0               age     float           1           10   \n",
      "2               1.0            income     float           1           10   \n",
      "3               1.0  experience_years     float           1           10   \n",
      "4               0.3            gender    string           1           10   \n",
      "\n",
      "   null_percentage  unique_count  \n",
      "0              0.0             4  \n",
      "1             10.0            10  \n",
      "2             10.0            10  \n",
      "3             10.0            10  \n",
      "4             10.0             3  \n",
      "\n",
      "Cleaned up ml_sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "# üÜï NEW in v0.4.5: Enhanced Pandas Integration with DataFrame outputs\n",
    "print(\"=== Enhanced Pandas Integration (New in v0.4.5) ===\")\n",
    "\n",
    "# Create a more comprehensive dataset for ML analysis\n",
    "ml_data = {\n",
    "    \"age\": [25, 32, 40, None, 18, 22, 45, 33, 28, 35],\n",
    "    \"income\": [30000, 50000, 70000, 45000, None, 22000, 80000, 55000, 42000, 65000],\n",
    "    \"experience_years\": [2, 8, 15, None, 0, 1, 20, 10, 5, 12],\n",
    "    \"gender\": [\"M\", \"F\", \"M\", \"F\", \"F\", None, \"M\", \"F\", \"M\", \"F\"],\n",
    "    \"city\": [\"NYC\", \"LA\", \"Chicago\", \"NYC\", \"Boston\", \"LA\", \"Chicago\", \"NYC\", \"Boston\", \"LA\"],\n",
    "    \"target\": [0, 1, 1, 0, 0, 0, 1, 1, 0, 1]  # Binary target for ML\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ml_data)\n",
    "print(\"Enhanced ML dataset created:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['target'].value_counts()}\")\n",
    "\n",
    "# Save to CSV for analysis\n",
    "csv_file = \"ml_sample_data.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Dataset saved to {csv_file}\")\n",
    "\n",
    "try:\n",
    "    # üÜï NEW: Enhanced pandas integration with DataFrame output\n",
    "    print(\"\\n=== Pandas DataFrame Integration (New in v0.4.5) ===\")\n",
    "    profiles_df = dp.analyze_csv_dataframe(csv_file)\n",
    "    print(f\"Profiles DataFrame shape: {profiles_df.shape}\")\n",
    "    print(\"Profiles DataFrame columns:\", profiles_df.columns.tolist())\n",
    "    print(\"\\nFirst few rows of profiles:\")\n",
    "    print(profiles_df.head())\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  Enhanced pandas integration not available (requires v0.4.5+)\")\n",
    "    print(\"Using standard analysis instead...\")\n",
    "    analysis_result = dp.analyze_csv_file(csv_file)\n",
    "    print(f\"Standard analysis completed: {len(analysis_result)} column profiles generated\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nCleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ML Readiness Assessment (Major New Feature in v0.4.5) ===\n",
      "ML dataset for readiness assessment:\n",
      "   feature_1  feature_2 category   timestamp  target_variable\n",
      "0        1.2      100.0        A  2023-01-01                0\n",
      "1        2.5      200.0        B  2023-01-02                1\n",
      "2        3.8      300.0        A  2023-01-03                0\n",
      "3        4.1      400.0        C  2023-01-04                1\n",
      "4        5.3        NaN        B  2023-01-05                1\n",
      "5        6.7      600.0        A  2023-01-06                0\n",
      "6        7.9      700.0        C  2023-01-07                1\n",
      "7        8.2      800.0        B  2023-01-08                0\n",
      "8        9.1      900.0        A  2023-01-09                1\n",
      "9       10.5     1000.0        B  2023-01-10                0\n",
      "\n",
      "=== ML Readiness Score (New in v0.4.5) ===\n",
      "ML Ready: True (Score: 95.9%)\n",
      "‚ÑπÔ∏è  ML readiness assessment not available (requires v0.4.5+)\n",
      "\n",
      "=== Feature Analysis DataFrame (New in v0.4.5) ===\n",
      "Features DataFrame shape: (5, 6)\n",
      "Features DataFrame:\n",
      "       column_name  ml_suitability                feature_type  \\\n",
      "0        timestamp             0.7  temporal_needs_engineering   \n",
      "1         category             0.5      high_cardinality_risky   \n",
      "2        feature_2             0.9               numeric_ready   \n",
      "3  target_variable             0.9               numeric_ready   \n",
      "4        feature_1             0.9               numeric_ready   \n",
      "\n",
      "  importance_potential                      potential_issues  \\\n",
      "0                 high                                         \n",
      "1               medium  High cardinality categorical feature   \n",
      "2                 high                                         \n",
      "3                 high                                         \n",
      "4                 high                                         \n",
      "\n",
      "                                encoding_suggestions  \n",
      "0  Extract year, month, day, day_of_week, Calcula...  \n",
      "1  Consider dimensionality reduction or feature h...  \n",
      "2          Consider standardization or normalization  \n",
      "3          Consider standardization or normalization  \n",
      "4          Consider standardization or normalization  \n",
      "\n",
      "Cleaned up ml_readiness_test.csv\n"
     ]
    }
   ],
   "source": [
    "# üÜï NEW in v0.4.5: ML Readiness Assessment System\n",
    "print(\"=== ML Readiness Assessment (Major New Feature in v0.4.5) ===\")\n",
    "\n",
    "# Create a comprehensive ML dataset for testing\n",
    "ml_dataset = {\n",
    "    \"feature_1\": [1.2, 2.5, 3.8, 4.1, 5.3, 6.7, 7.9, 8.2, 9.1, 10.5],\n",
    "    \"feature_2\": [100, 200, 300, 400, None, 600, 700, 800, 900, 1000],\n",
    "    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\", \"A\", \"B\"],\n",
    "    \"timestamp\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\",\n",
    "                 \"2023-01-06\", \"2023-01-07\", \"2023-01-08\", \"2023-01-09\", \"2023-01-10\"],\n",
    "    \"target_variable\": [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "ml_df = pd.DataFrame(ml_dataset)\n",
    "print(\"ML dataset for readiness assessment:\")\n",
    "print(ml_df)\n",
    "\n",
    "# Save for analysis\n",
    "ml_csv = \"ml_readiness_test.csv\"\n",
    "ml_df.to_csv(ml_csv, index=False)\n",
    "\n",
    "try:\n",
    "    # üÜï NEW: ML Readiness Assessment\n",
    "    print(\"\\n=== ML Readiness Score (New in v0.4.5) ===\")\n",
    "    ml_score = dp.ml_readiness_score(ml_csv)\n",
    "    print(f\"ML Ready: {ml_score.is_ml_ready()} (Score: {ml_score.overall_score:.1f}%)\")\n",
    "    print(f\"Feature analysis completed for {len(ml_score.features)} features\")\n",
    "    \n",
    "    # Display feature analysis\n",
    "    print(\"\\n=== Feature Analysis ===\")\n",
    "    for i, feature in enumerate(ml_score.features):\n",
    "        print(f\"Feature {i+1}: {feature.name}\")\n",
    "        print(f\"  - Type: {feature.feature_type}\")\n",
    "        print(f\"  - ML Ready: {feature.is_ml_ready}\")\n",
    "        if hasattr(feature, 'recommendations'):\n",
    "            print(f\"  - Recommendations: {len(feature.recommendations)} items\")\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  ML readiness assessment not available (requires v0.4.5+)\")\n",
    "\n",
    "try:\n",
    "    # üÜï NEW: Feature analysis DataFrame\n",
    "    print(\"\\n=== Feature Analysis DataFrame (New in v0.4.5) ===\")\n",
    "    features_df = dp.feature_analysis_dataframe(ml_csv)\n",
    "    print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "    print(\"Features DataFrame:\")\n",
    "    print(features_df)\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  Feature analysis DataFrame not available (requires v0.4.5+)\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(ml_csv):\n",
    "    os.remove(ml_csv)\n",
    "    print(f\"\\nCleaned up {ml_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to sample_data.csv\n",
      "\n",
      "=== Basic Analysis ===\n",
      "Analysis completed: 6 column profiles generated\n",
      "\n",
      "=== Column Profile Details ===\n",
      "\n",
      "Column 1:\n",
      "  data_type: string\n",
      "  name: gender\n",
      "  null_count: 1\n",
      "  null_percentage: 10.0\n",
      "  total_count: 10\n",
      "  unique_count: 3\n",
      "  uniqueness_ratio: 0.3\n",
      "\n",
      "Column 2:\n",
      "  data_type: integer\n",
      "  name: target\n",
      "  null_count: 0\n",
      "  null_percentage: 0.0\n",
      "  total_count: 10\n",
      "  unique_count: 2\n",
      "  uniqueness_ratio: 0.2\n",
      "\n",
      "Column 3:\n",
      "  data_type: float\n",
      "  name: age\n",
      "  null_count: 1\n",
      "  null_percentage: 10.0\n",
      "  total_count: 10\n",
      "  unique_count: 10\n",
      "  uniqueness_ratio: 1.0\n",
      "\n",
      "Column 4:\n",
      "  data_type: string\n",
      "  name: city\n",
      "  null_count: 0\n",
      "  null_percentage: 0.0\n",
      "  total_count: 10\n",
      "  unique_count: 4\n",
      "  uniqueness_ratio: 0.4\n",
      "\n",
      "Column 5:\n",
      "  data_type: float\n",
      "  name: income\n",
      "  null_count: 1\n",
      "  null_percentage: 10.0\n",
      "  total_count: 10\n",
      "  unique_count: 10\n",
      "  uniqueness_ratio: 1.0\n",
      "\n",
      "Column 6:\n",
      "  data_type: float\n",
      "  name: experience_years\n",
      "  null_count: 1\n",
      "  null_percentage: 10.0\n",
      "  total_count: 10\n",
      "  unique_count: 10\n",
      "  uniqueness_ratio: 1.0\n",
      "\n",
      "Cleaned up sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to CSV for dataprof analysis\n",
    "csv_file = \"sample_data.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Dataset saved to {csv_file}\")\n",
    "\n",
    "# Basic analysis with dataprof\n",
    "try:\n",
    "    print(\"\\n=== Basic Analysis ===\")\n",
    "    analysis_result = dp.analyze_csv_file(csv_file)\n",
    "    print(f\"Analysis completed: {len(analysis_result)} column profiles generated\")\n",
    "    \n",
    "    # Inspect the column profiles\n",
    "    print(\"\\n=== Column Profile Details ===\")\n",
    "    for i, profile in enumerate(analysis_result):\n",
    "        print(f\"\\nColumn {i+1}:\")\n",
    "        # Check available attributes of the profile object\n",
    "        attrs = [attr for attr in dir(profile) if not attr.startswith('_')]\n",
    "        for attr in attrs[:10]:  # Show first 10 attributes\n",
    "            try:\n",
    "                value = getattr(profile, attr)\n",
    "                if not callable(value):\n",
    "                    print(f\"  {attr}: {value}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nCleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Quality Analysis ===\n",
      "Quality score: 45.0\n",
      "Total rows: 10\n",
      "Total columns: 6\n",
      "Scan time: 94 ms\n",
      "Issues found: 7\n",
      "\n",
      "Quality issues detected:\n",
      "1. 6 duplicate values in column 'city' (Column: city, Severity: low)\n",
      "2. 8 duplicate values in column 'target' (Column: target, Severity: low)\n",
      "3. 1 null values (10%) in column 'age' (Column: age, Severity: medium)\n",
      "4. 1 null values (10%) in column 'income' (Column: income, Severity: medium)\n",
      "5. 1 null values (10%) in column 'experience_years' (Column: experience_years, Severity: medium)\n",
      "6. 1 null values (10%) in column 'gender' (Column: gender, Severity: medium)\n",
      "7. 7 duplicate values in column 'gender' (Column: gender, Severity: low)\n",
      "\n",
      "Cleaned up sample_data_quality.csv\n"
     ]
    }
   ],
   "source": [
    "# Quality analysis\n",
    "csv_file = \"sample_data_quality.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "try:\n",
    "    print(\"=== Quality Analysis ===\")\n",
    "    quality_report = dp.analyze_csv_with_quality(csv_file)\n",
    "    \n",
    "    print(f\"Quality score: {quality_report.quality_score()}\")\n",
    "    print(f\"Total rows: {quality_report.total_rows}\")\n",
    "    print(f\"Total columns: {quality_report.total_columns}\")\n",
    "    print(f\"Scan time: {quality_report.scan_time_ms} ms\")\n",
    "    print(f\"Issues found: {len(quality_report.issues)}\")\n",
    "    \n",
    "    if quality_report.issues:\n",
    "        print(\"\\nQuality issues detected:\")\n",
    "        for i, issue in enumerate(quality_report.issues, 1):\n",
    "            print(f\"{i}. {issue.description} (Column: {issue.column}, Severity: {issue.severity})\")\n",
    "    else:\n",
    "        print(\"No quality issues detected!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during quality analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nCleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2557100136.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mThis notebook demonstrated key dataprof capabilities including major new features in v0.4.5:\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Summary - DataProf v0.4.5 Capabilities\n",
    "\n",
    "This notebook demonstrated key dataprof capabilities including major new features in v0.4.5:\n",
    "\n",
    "### üÜï New Features in v0.4.5:\n",
    "- **`ml_readiness_score()`** - Complete ML readiness assessment with feature analysis\n",
    "- **`analyze_csv_dataframe()`** - Enhanced pandas integration with DataFrame outputs\n",
    "- **`feature_analysis_dataframe()`** - ML feature analysis in DataFrame format\n",
    "- **`configure_logging()`** - Python logging integration with configurable levels\n",
    "- **Context Managers** - `PyBatchAnalyzer`, `PyMlAnalyzer`, `PyCsvProcessor` for resource management\n",
    "- **Enhanced Security** - Comprehensive SQL injection protection and input validation\n",
    "\n",
    "### Core Functions (v0.4.1-0.4.5):\n",
    "- `analyze_csv_file()` - Basic CSV column profiling\n",
    "- `analyze_csv_with_quality()` - Quality assessment with scoring\n",
    "- `batch_analyze_glob()` - Batch processing of multiple files\n",
    "- `analyze_json_file()` - JSON file analysis\n",
    "\n",
    "### Key Features:\n",
    "- **Fast analysis** of CSV data quality\n",
    "- **ML readiness assessment** with comprehensive scoring and recommendations\n",
    "- **Missing value detection** and quantification\n",
    "- **Quality scoring** for datasets with severity-based issues\n",
    "- **Batch processing** for multiple files with progress tracking\n",
    "- **ML workflow integration** for preprocessing validation\n",
    "- **Enhanced pandas integration** with DataFrame outputs\n",
    "- **Context managers** for proper resource management\n",
    "- **Security hardening** with input validation and error sanitization\n",
    "\n",
    "### ML/AI Enhancements:\n",
    "- **Feature type detection** (numeric_ready, categorical_needs_encoding, temporal_needs_engineering)\n",
    "- **Blocking issues detection** (missing targets, all-null features, data leakage)\n",
    "- **ML preprocessing recommendations** with priority levels\n",
    "- **Scikit-learn integration** examples and pipeline building\n",
    "- **Jupyter notebook support** with rich HTML displays\n",
    "\n",
    "### Next Steps:\n",
    "- Explore database ML readiness with `profile_database_with_ml()`\n",
    "- Try directory-wide analysis with enhanced batch processing\n",
    "- Implement automated ML preprocessing pipelines\n",
    "- Generate comprehensive quality and ML readiness reports\n",
    "- Leverage context managers for production data workflows\n",
    "- Use enhanced security features for production deployments\n",
    "\n",
    "### Version Upgrade Benefits:\n",
    "- **Comprehensive ML readiness** assessment for data science workflows\n",
    "- **Enhanced pandas integration** for data analysis pipelines\n",
    "- **Resource management** with context managers\n",
    "- **Security improvements** for production use\n",
    "- **Performance optimizations** and reliability enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Batch Analysis Example ===\n",
      "Created batch_test_0.csv\n",
      "Created batch_test_1.csv\n",
      "Created batch_test_2.csv\n",
      "\n",
      "Running batch analysis...\n",
      "Batch analysis completed, result type: <class 'builtins.PyBatchResult'>\n",
      "\n",
      "Batch result attributes:\n",
      "  average_quality_score: 98.33333333333333\n",
      "  failed_files: 0\n",
      "  processed_files: 3\n",
      "  total_duration_secs: 0.1200045\n",
      "  total_quality_issues: 3\n",
      "\n",
      "Cleaned up batch test files\n"
     ]
    }
   ],
   "source": [
    "# Batch analysis example\n",
    "print(\"=== Batch Analysis Example ===\")\n",
    "\n",
    "# Create multiple test files\n",
    "test_files = []\n",
    "for i in range(3):\n",
    "    # Create slightly different datasets\n",
    "    sample_data = {\n",
    "        \"value1\": [i*10 + j for j in range(5)],\n",
    "        \"value2\": [j*2 + i for j in range(5)],\n",
    "        \"category\": [f\"Type_{j%2}\" for j in range(5)]\n",
    "    }\n",
    "    \n",
    "    filename = f\"batch_test_{i}.csv\"\n",
    "    pd.DataFrame(sample_data).to_csv(filename, index=False)\n",
    "    test_files.append(filename)\n",
    "    print(f\"Created {filename}\")\n",
    "\n",
    "try:\n",
    "    # Use batch analysis\n",
    "    print(\"\\nRunning batch analysis...\")\n",
    "    batch_result = dp.batch_analyze_glob(\"batch_test_*.csv\")\n",
    "    \n",
    "    print(f\"Batch analysis completed, result type: {type(batch_result)}\")\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"\\nBatch result attributes:\")\n",
    "    attrs = [attr for attr in dir(batch_result) if not attr.startswith('_')]\n",
    "    for attr in attrs[:10]:  # Show first 10 attributes\n",
    "        try:\n",
    "            value = getattr(batch_result, attr)\n",
    "            if not callable(value):\n",
    "                print(f\"  {attr}: {value}\")\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up test files\n",
    "for filename in test_files:\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "print(\"\\nCleaned up batch test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ML Workflow with Iris Dataset ===\n",
      "Iris dataset shape: (150, 5)\n",
      "\n",
      "First few rows:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "=== Dataprof Analysis on Iris ===\n",
      "Iris analysis: 5 column profiles\n",
      "Iris quality score: 70.0\n",
      "Iris issues: 6\n",
      "Quality issues in Iris dataset:\n",
      "- 115 duplicate values in column 'sepal length (cm)'\n",
      "- 147 duplicate values in column 'target'\n",
      "- 128 duplicate values in column 'petal width (cm)'\n",
      "- 127 duplicate values in column 'sepal width (cm)'\n",
      "- 1 outlier values in column 'sepal width (cm)' (threshold: 3): [\"Row 16: 4.4\"]\n",
      "- 107 duplicate values in column 'petal length (cm)'\n",
      "\n",
      "=== Training ML Model ===\n",
      "Training accuracy: 1.000\n",
      "Test accuracy: 1.000\n",
      "\n",
      "Cleaned up iris_data.csv\n",
      "\n",
      "üéâ Dataprof demo completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ML workflow example with Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"=== ML Workflow with Iris Dataset ===\")\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.frame\n",
    "print(f\"Iris dataset shape: {iris_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "# Save for dataprof analysis\n",
    "iris_file = \"iris_data.csv\"\n",
    "iris_df.to_csv(iris_file, index=False)\n",
    "\n",
    "try:\n",
    "    # Analyze with dataprof\n",
    "    print(\"\\n=== Dataprof Analysis on Iris ===\")\n",
    "    iris_profiles = dp.analyze_csv_file(iris_file)\n",
    "    print(f\"Iris analysis: {len(iris_profiles)} column profiles\")\n",
    "    \n",
    "    iris_quality = dp.analyze_csv_with_quality(iris_file)\n",
    "    print(f\"Iris quality score: {iris_quality.quality_score()}\")\n",
    "    print(f\"Iris issues: {len(iris_quality.issues)}\")\n",
    "    \n",
    "    if iris_quality.issues:\n",
    "        print(\"Quality issues in Iris dataset:\")\n",
    "        for issue in iris_quality.issues:\n",
    "            print(f\"- {issue.description}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No quality issues found in Iris dataset!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing Iris: {e}\")\n",
    "\n",
    "# ML training\n",
    "print(\"\\n=== Training ML Model ===\")\n",
    "X = iris_df.drop('target', axis=1)\n",
    "y = iris_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(f\"Training accuracy: {train_score:.3f}\")\n",
    "print(f\"Test accuracy: {test_score:.3f}\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(iris_file):\n",
    "    os.remove(iris_file)\n",
    "    print(f\"\\nCleaned up {iris_file}\")\n",
    "\n",
    "print(\"\\nüéâ Dataprof demo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key dataprof capabilities:\n",
    "\n",
    "### Core Functions Used (v0.4.1):\n",
    "- `analyze_csv_file()` - Basic CSV column profiling\n",
    "- `analyze_csv_with_quality()` - Quality assessment with scoring\n",
    "- `batch_analyze_glob()` - Batch processing of multiple files\n",
    "- `analyze_json_file()` - JSON file analysis (available)\n",
    "\n",
    "### Key Features:\n",
    "- **Fast analysis** of CSV data quality\n",
    "- **Missing value detection** and quantification\n",
    "- **Quality scoring** for datasets\n",
    "- **Batch processing** for multiple files\n",
    "- **ML workflow integration** for preprocessing validation\n",
    "\n",
    "### Next Steps:\n",
    "- Explore JSON analysis with `analyze_json_file()`\n",
    "- Try directory-wide analysis with `batch_analyze_directory()`\n",
    "- Implement data validation pipelines\n",
    "- Generate automated quality reports\n",
    "- Upgrade to newer dataprof versions for additional features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-short",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
