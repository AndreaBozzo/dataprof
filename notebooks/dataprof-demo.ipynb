{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring dataprof v0.4.5 for Data Engineering & ML Workflows\n",
    "\n",
    "This notebook provides a comprehensive hands-on introduction to the [`dataprof`](https://github.com/AndreaBozzo/dataprof) library v0.4.5.  \n",
    "The goal is to demonstrate how it can be used for **data profiling**, **quality checks**, **ML readiness assessment**, and as a helper in **ML pipelines**.\n",
    "\n",
    "## üÜï New in v0.4.5:\n",
    "- **ML Readiness Assessment System** with comprehensive scoring\n",
    "- **Enhanced Pandas Integration** with DataFrame outputs  \n",
    "- **Context Managers** for resource management\n",
    "- **Security enhancements** and comprehensive fixes\n",
    "- **Python Logging Integration** with configurable levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if needed\n",
    "# %pip install dataprof pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataprof as dp\n",
    "import os\n",
    "\n",
    "print(f\"Dataprof version: {getattr(dp, '__version__', 'Unknown')}\")\n",
    "print(\"Available functions:\", [f for f in dir(dp) if not f.startswith('_') and callable(getattr(dp, f))])\n",
    "\n",
    "# üÜï NEW in v0.4.5: Configure logging\n",
    "print(\"\\n=== Configuring Python Logging (New in v0.4.5) ===\")\n",
    "try:\n",
    "    dp.configure_logging(level=\"INFO\")\n",
    "    print(\"‚úÖ Python logging configured successfully!\")\n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  Logging configuration not available (requires v0.4.5+)\")\n",
    "\n",
    "print(\"Setup completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with some data quality issues\n",
    "data = {\n",
    "    \"age\": [25, 32, 40, None, 18, 22, 45, 33],\n",
    "    \"income\": [30000, 50000, 70000, 45000, None, 22000, 80000, 55000],\n",
    "    \"gender\": [\"M\", \"F\", \"M\", \"F\", \"F\", None, \"M\", \"F\"],\n",
    "    \"city\": [\"NYC\", \"LA\", \"Chicago\", \"NYC\", \"Boston\", \"LA\", \"Chicago\", \"NYC\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Sample dataset created:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Missing values per column:\\n{df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üÜï NEW in v0.4.5: Enhanced Pandas Integration with DataFrame outputs\n",
    "print(\"=== Enhanced Pandas Integration (New in v0.4.5) ===\")\n",
    "\n",
    "# Create a more comprehensive dataset for ML analysis\n",
    "ml_data = {\n",
    "    \"age\": [25, 32, 40, None, 18, 22, 45, 33, 28, 35],\n",
    "    \"income\": [30000, 50000, 70000, 45000, None, 22000, 80000, 55000, 42000, 65000],\n",
    "    \"experience_years\": [2, 8, 15, None, 0, 1, 20, 10, 5, 12],\n",
    "    \"gender\": [\"M\", \"F\", \"M\", \"F\", \"F\", None, \"M\", \"F\", \"M\", \"F\"],\n",
    "    \"city\": [\"NYC\", \"LA\", \"Chicago\", \"NYC\", \"Boston\", \"LA\", \"Chicago\", \"NYC\", \"Boston\", \"LA\"],\n",
    "    \"target\": [0, 1, 1, 0, 0, 0, 1, 1, 0, 1]  # Binary target for ML\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ml_data)\n",
    "print(\"Enhanced ML dataset created:\")\n",
    "print(df)\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['target'].value_counts()}\")\n",
    "\n",
    "# Save to CSV for analysis\n",
    "csv_file = \"ml_sample_data.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Dataset saved to {csv_file}\")\n",
    "\n",
    "try:\n",
    "    # üÜï NEW: Enhanced pandas integration with DataFrame output\n",
    "    print(\"\\n=== Pandas DataFrame Integration (New in v0.4.5) ===\")\n",
    "    profiles_df = dp.analyze_csv_dataframe(csv_file)\n",
    "    print(f\"Profiles DataFrame shape: {profiles_df.shape}\")\n",
    "    print(\"Profiles DataFrame columns:\", profiles_df.columns.tolist())\n",
    "    print(\"\\nFirst few rows of profiles:\")\n",
    "    print(profiles_df.head())\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  Enhanced pandas integration not available (requires v0.4.5+)\")\n",
    "    print(\"Using standard analysis instead...\")\n",
    "    analysis_result = dp.analyze_csv_file(csv_file)\n",
    "    print(f\"Standard analysis completed: {len(analysis_result)} column profiles generated\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nCleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üÜï NEW in v0.4.5: ML Readiness Assessment System\n",
    "print(\"=== ML Readiness Assessment (Major New Feature in v0.4.5) ===\")\n",
    "\n",
    "# Create a comprehensive ML dataset for testing\n",
    "ml_dataset = {\n",
    "    \"feature_1\": [1.2, 2.5, 3.8, 4.1, 5.3, 6.7, 7.9, 8.2, 9.1, 10.5],\n",
    "    \"feature_2\": [100, 200, 300, 400, None, 600, 700, 800, 900, 1000],\n",
    "    \"category\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"A\", \"C\", \"B\", \"A\", \"B\"],\n",
    "    \"timestamp\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\", \"2023-01-05\",\n",
    "                 \"2023-01-06\", \"2023-01-07\", \"2023-01-08\", \"2023-01-09\", \"2023-01-10\"],\n",
    "    \"target_variable\": [0, 1, 0, 1, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "ml_df = pd.DataFrame(ml_dataset)\n",
    "print(\"ML dataset for readiness assessment:\")\n",
    "print(ml_df)\n",
    "\n",
    "# Save for analysis\n",
    "ml_csv = \"ml_readiness_test.csv\"\n",
    "ml_df.to_csv(ml_csv, index=False)\n",
    "\n",
    "try:\n",
    "    # üÜï NEW: ML Readiness Assessment\n",
    "    print(\"\\n=== ML Readiness Score (New in v0.4.5) ===\")\n",
    "    ml_score = dp.ml_readiness_score(ml_csv)\n",
    "    print(f\"ML Ready: {ml_score.is_ml_ready()} (Score: {ml_score.overall_score:.1f}%)\")\n",
    "    print(f\"Feature analysis completed for {len(ml_score.features)} features\")\n",
    "    \n",
    "    # Display feature analysis\n",
    "    print(\"\\n=== Feature Analysis ===\")\n",
    "    for i, feature in enumerate(ml_score.features):\n",
    "        print(f\"Feature {i+1}: {feature.name}\")\n",
    "        print(f\"  - Type: {feature.feature_type}\")\n",
    "        print(f\"  - ML Ready: {feature.is_ml_ready}\")\n",
    "        if hasattr(feature, 'recommendations'):\n",
    "            print(f\"  - Recommendations: {len(feature.recommendations)} items\")\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  ML readiness assessment not available (requires v0.4.5+)\")\n",
    "\n",
    "try:\n",
    "    # üÜï NEW: Feature analysis DataFrame\n",
    "    print(\"\\n=== Feature Analysis DataFrame (New in v0.4.5) ===\")\n",
    "    features_df = dp.feature_analysis_dataframe(ml_csv)\n",
    "    print(f\"Features DataFrame shape: {features_df.shape}\")\n",
    "    print(\"Features DataFrame:\")\n",
    "    print(features_df)\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"‚ÑπÔ∏è  Feature analysis DataFrame not available (requires v0.4.5+)\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(ml_csv):\n",
    "    os.remove(ml_csv)\n",
    "    print(f\"\\nCleaned up {ml_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for dataprof analysis\n",
    "csv_file = \"sample_data.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Dataset saved to {csv_file}\")\n",
    "\n",
    "# Basic analysis with dataprof\n",
    "try:\n",
    "    print(\"\\n=== Basic Analysis ===\")\n",
    "    analysis_result = dp.analyze_csv_file(csv_file)\n",
    "    print(f\"Analysis completed: {len(analysis_result)} column profiles generated\")\n",
    "    \n",
    "    # Inspect the column profiles\n",
    "    print(\"\\n=== Column Profile Details ===\")\n",
    "    for i, profile in enumerate(analysis_result):\n",
    "        print(f\"\\nColumn {i+1}:\")\n",
    "        # Check available attributes of the profile object\n",
    "        attrs = [attr for attr in dir(profile) if not attr.startswith('_')]\n",
    "        for attr in attrs[:10]:  # Show first 10 attributes\n",
    "            try:\n",
    "                value = getattr(profile, attr)\n",
    "                if not callable(value):\n",
    "                    print(f\"  {attr}: {value}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nCleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality analysis\n",
    "csv_file = \"sample_data_quality.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "try:\n",
    "    print(\"=== Quality Analysis ===\")\n",
    "    quality_report = dp.analyze_csv_with_quality(csv_file)\n",
    "    \n",
    "    print(f\"Quality score: {quality_report.quality_score()}\")\n",
    "    print(f\"Total rows: {quality_report.total_rows}\")\n",
    "    print(f\"Total columns: {quality_report.total_columns}\")\n",
    "    print(f\"Scan time: {quality_report.scan_time_ms} ms\")\n",
    "    print(f\"Issues found: {len(quality_report.issues)}\")\n",
    "    \n",
    "    if quality_report.issues:\n",
    "        print(\"\\nQuality issues detected:\")\n",
    "        for i, issue in enumerate(quality_report.issues, 1):\n",
    "            print(f\"{i}. {issue.description} (Column: {issue.column}, Severity: {issue.severity})\")\n",
    "    else:\n",
    "        print(\"No quality issues detected!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during quality analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_file):\n",
    "    os.remove(csv_file)\n",
    "    print(f\"\\nCleaned up {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Summary - DataProf v0.4.5 Capabilities\n",
    "\n",
    "This notebook demonstrated key dataprof capabilities including major new features in v0.4.5:\n",
    "\n",
    "### üÜï New Features in v0.4.5:\n",
    "- **`ml_readiness_score()`** - Complete ML readiness assessment with feature analysis\n",
    "- **`analyze_csv_dataframe()`** - Enhanced pandas integration with DataFrame outputs\n",
    "- **`feature_analysis_dataframe()`** - ML feature analysis in DataFrame format\n",
    "- **`configure_logging()`** - Python logging integration with configurable levels\n",
    "- **Context Managers** - `PyBatchAnalyzer`, `PyMlAnalyzer`, `PyCsvProcessor` for resource management\n",
    "- **Enhanced Security** - Comprehensive SQL injection protection and input validation\n",
    "\n",
    "### Core Functions (v0.4.1-0.4.5):\n",
    "- `analyze_csv_file()` - Basic CSV column profiling\n",
    "- `analyze_csv_with_quality()` - Quality assessment with scoring\n",
    "- `batch_analyze_glob()` - Batch processing of multiple files\n",
    "- `analyze_json_file()` - JSON file analysis\n",
    "\n",
    "### Key Features:\n",
    "- **Fast analysis** of CSV data quality\n",
    "- **ML readiness assessment** with comprehensive scoring and recommendations\n",
    "- **Missing value detection** and quantification\n",
    "- **Quality scoring** for datasets with severity-based issues\n",
    "- **Batch processing** for multiple files with progress tracking\n",
    "- **ML workflow integration** for preprocessing validation\n",
    "- **Enhanced pandas integration** with DataFrame outputs\n",
    "- **Context managers** for proper resource management\n",
    "- **Security hardening** with input validation and error sanitization\n",
    "\n",
    "### ML/AI Enhancements:\n",
    "- **Feature type detection** (numeric_ready, categorical_needs_encoding, temporal_needs_engineering)\n",
    "- **Blocking issues detection** (missing targets, all-null features, data leakage)\n",
    "- **ML preprocessing recommendations** with priority levels\n",
    "- **Scikit-learn integration** examples and pipeline building\n",
    "- **Jupyter notebook support** with rich HTML displays\n",
    "\n",
    "### Next Steps:\n",
    "- Explore database ML readiness with `profile_database_with_ml()`\n",
    "- Try directory-wide analysis with enhanced batch processing\n",
    "- Implement automated ML preprocessing pipelines\n",
    "- Generate comprehensive quality and ML readiness reports\n",
    "- Leverage context managers for production data workflows\n",
    "- Use enhanced security features for production deployments\n",
    "\n",
    "### Version Upgrade Benefits:\n",
    "- **Comprehensive ML readiness** assessment for data science workflows\n",
    "- **Enhanced pandas integration** for data analysis pipelines\n",
    "- **Resource management** with context managers\n",
    "- **Security improvements** for production use\n",
    "- **Performance optimizations** and reliability enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch analysis example\n",
    "print(\"=== Batch Analysis Example ===\")\n",
    "\n",
    "# Create multiple test files\n",
    "test_files = []\n",
    "for i in range(3):\n",
    "    # Create slightly different datasets\n",
    "    sample_data = {\n",
    "        \"value1\": [i*10 + j for j in range(5)],\n",
    "        \"value2\": [j*2 + i for j in range(5)],\n",
    "        \"category\": [f\"Type_{j%2}\" for j in range(5)]\n",
    "    }\n",
    "    \n",
    "    filename = f\"batch_test_{i}.csv\"\n",
    "    pd.DataFrame(sample_data).to_csv(filename, index=False)\n",
    "    test_files.append(filename)\n",
    "    print(f\"Created {filename}\")\n",
    "\n",
    "try:\n",
    "    # Use batch analysis\n",
    "    print(\"\\nRunning batch analysis...\")\n",
    "    batch_result = dp.batch_analyze_glob(\"batch_test_*.csv\")\n",
    "    \n",
    "    print(f\"Batch analysis completed, result type: {type(batch_result)}\")\n",
    "    \n",
    "    # Show results\n",
    "    print(f\"\\nBatch result attributes:\")\n",
    "    attrs = [attr for attr in dir(batch_result) if not attr.startswith('_')]\n",
    "    for attr in attrs[:10]:  # Show first 10 attributes\n",
    "        try:\n",
    "            value = getattr(batch_result, attr)\n",
    "            if not callable(value):\n",
    "                print(f\"  {attr}: {value}\")\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Error during batch analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up test files\n",
    "for filename in test_files:\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "print(\"\\nCleaned up batch test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML workflow example with Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "print(\"=== ML Workflow with Iris Dataset ===\")\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "iris_df = iris.frame\n",
    "print(f\"Iris dataset shape: {iris_df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "# Save for dataprof analysis\n",
    "iris_file = \"iris_data.csv\"\n",
    "iris_df.to_csv(iris_file, index=False)\n",
    "\n",
    "try:\n",
    "    # Analyze with dataprof\n",
    "    print(\"\\n=== Dataprof Analysis on Iris ===\")\n",
    "    iris_profiles = dp.analyze_csv_file(iris_file)\n",
    "    print(f\"Iris analysis: {len(iris_profiles)} column profiles\")\n",
    "    \n",
    "    iris_quality = dp.analyze_csv_with_quality(iris_file)\n",
    "    print(f\"Iris quality score: {iris_quality.quality_score()}\")\n",
    "    print(f\"Iris issues: {len(iris_quality.issues)}\")\n",
    "    \n",
    "    if iris_quality.issues:\n",
    "        print(\"Quality issues in Iris dataset:\")\n",
    "        for issue in iris_quality.issues:\n",
    "            print(f\"- {issue.description}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No quality issues found in Iris dataset!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing Iris: {e}\")\n",
    "\n",
    "# ML training\n",
    "print(\"\\n=== Training ML Model ===\")\n",
    "X = iris_df.drop('target', axis=1)\n",
    "y = iris_df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(f\"Training accuracy: {train_score:.3f}\")\n",
    "print(f\"Test accuracy: {test_score:.3f}\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(iris_file):\n",
    "    os.remove(iris_file)\n",
    "    print(f\"\\nCleaned up {iris_file}\")\n",
    "\n",
    "print(\"\\nüéâ Dataprof demo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key dataprof capabilities:\n",
    "\n",
    "### Core Functions Used (v0.4.1):\n",
    "- `analyze_csv_file()` - Basic CSV column profiling\n",
    "- `analyze_csv_with_quality()` - Quality assessment with scoring\n",
    "- `batch_analyze_glob()` - Batch processing of multiple files\n",
    "- `analyze_json_file()` - JSON file analysis (available)\n",
    "\n",
    "### Key Features:\n",
    "- **Fast analysis** of CSV data quality\n",
    "- **Missing value detection** and quantification\n",
    "- **Quality scoring** for datasets\n",
    "- **Batch processing** for multiple files\n",
    "- **ML workflow integration** for preprocessing validation\n",
    "\n",
    "### Next Steps:\n",
    "- Explore JSON analysis with `analyze_json_file()`\n",
    "- Try directory-wide analysis with `batch_analyze_directory()`\n",
    "- Implement data validation pipelines\n",
    "- Generate automated quality reports\n",
    "- Upgrade to newer dataprof versions for additional features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-short",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
